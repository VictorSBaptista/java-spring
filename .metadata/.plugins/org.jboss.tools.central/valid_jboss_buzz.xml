<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Introduction to the Node.js reference architecture, Part 3: Code consistency</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/CgZ9PJuRfJc/introduction-to-the-node-js-reference-architecture-part-2-code-consistency" /><author><name>Lucas Holmquist</name></author><id>cfa54527-158a-46bc-a33d-9768f9a42b70</id><updated>2021-05-17T03:00:00Z</updated><published>2021-05-17T03:00:00Z</published><summary type="html">&lt;p&gt;Welcome back to our ongoing series about the &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; reference architecture. &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview/"&gt;Part 1&lt;/a&gt; introduced what the Node.js reference architecture is all about, and &lt;a href="https://developer.ibm.com/languages/node-js/blogs/nodejs-reference-architectire-pino-for-logging/"&gt;Part 2&lt;/a&gt; took a look at logging. In this article, we will dive into code consistency and how to enforce it with a linter tool like ESLint.&lt;/p&gt; &lt;h2&gt;Why code consistency matters&lt;/h2&gt; &lt;p&gt;One critical aspect of working on JavaScript projects effectively as a team is having consistency in the formatting of your code. This ensures that when different team members collaborate on the shared codebase, they know what coding patterns to expect, allowing them to work more efficiently. A lack of consistency increases the learning curve for developers and can potentially detract from the main project goal.&lt;/p&gt; &lt;p&gt;When the Node.js teams at Red Hat and IBM started the discussion on code consistency, it quickly became apparent that this is an area where people have strong opinions, and one size does not fit all. It's amazing how much time you can spend talking about the right place for a bracket!&lt;/p&gt; &lt;p&gt;The one thing we could agree on, though, is the importance of using a consistent style within a project and enforcing it through automation.&lt;/p&gt; &lt;h2&gt;ESLint&lt;/h2&gt; &lt;p&gt;In surveying the tools used across Red Hat and IBM to check and enforce code consistency, &lt;a href="https://eslint.org/"&gt;ESLint&lt;/a&gt; quickly surfaced as the most popular choice. This configurable linter tool analyzes code to identify JavaScript patterns and maintain quality.&lt;/p&gt; &lt;p&gt;While we found that different teams used different code styles, many of them reported that they used ESLint to get the job done. ESLint is an &lt;a href="https://developers.redhat.com/topics/open-source/"&gt;open source&lt;/a&gt; project hosted by the &lt;a href="https://openjsf.org/"&gt;OpenJS Foundation&lt;/a&gt;, confirming it as a solid choice with open governance. We know we'll always have the opportunity to contribute fixes and get involved with the project.&lt;/p&gt; &lt;p&gt;ESLint comes with many pre-existing code style configurations that you can easily add to your projects. Using one of these shareable configurations has many benefits. By using an existing config, you can avoid "reinventing the wheel"; someone else has probably already created the configuration you are looking for. Another advantage is that new team members (or open source contributors) might already be familiar with the config you are using, enabling them to get up to speed more quickly.&lt;/p&gt; &lt;p&gt;Here are a few common configurations to help you get started:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://www.npmjs.com/package/eslint-config-airbnb-standard"&gt;&lt;code&gt;eslint-config-airbnb-standard&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.npmjs.com/package/eslint-config-semistandard"&gt;&lt;code&gt;eslint-config-semistandard&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.npmjs.com/package/eslint-config-standard"&gt;&lt;code&gt;eslint-config-standard&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/prettier/prettier-eslint"&gt;&lt;code&gt;eslint-config-prettier&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;A complete list can be found on npmjs.org &lt;a href="https://www.npmjs.com/search?q=eslint-config-&amp;ranking=popularity"&gt;using this query&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Note we do not recommend any particular code style or ESLint config. It's more important that you choose one standard and that you apply it consistently across your organization. If that is not possible, then you should at least make sure it's used consistently across related projects.&lt;/p&gt; &lt;p&gt;At this point, I must admit we did not really spend &lt;em&gt;that&lt;/em&gt; much time talking about where the brackets should go. But that is one of the reasons we suggest looking at one of the existing configs: Adopting existing best practices saves a lot of time (and arguments) so you can spend that time coding instead.&lt;/p&gt; &lt;h3&gt;Adding ESLint to your Node.js project&lt;/h3&gt; &lt;p&gt;Based on the advice in the reference architecture, the Red Hat Node.js team recently updated the &lt;a href="https://github.com/nodeshift"&gt;NodeShift project&lt;/a&gt; to use ESLint.&lt;/p&gt; &lt;p&gt;Adding ESLint to your project is a pretty straightforward process. In fact, ESLint has a wizard that you can run on the command line interface to get you started. You can run:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ npx eslint --init &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and then follow the prompts. This post won’t go into the specifics of the &lt;code&gt;init&lt;/code&gt; wizard, but you can find more information in the &lt;a href="https://eslint.org/docs/user-guide/getting-started"&gt;ESLint documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Our team likes using semi-colons, so we decided to use the &lt;a href="https://www.npmjs.com/package/eslint-config-semistandard"&gt;&lt;code&gt;semistandard&lt;/code&gt; config&lt;/a&gt;. It was easy to install by running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ npx install-peerdeps --dev eslint-config-semistandard&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then, in our &lt;a href="https://github.com/nodeshift/nodeshift/blob/main/.eslintrc.json#L2"&gt;&lt;code&gt;.eslintrc.json&lt;/code&gt;&lt;/a&gt; file, we made sure to extend &lt;code&gt;semistandard&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;{ "extends": "semistandard", "rules": { "prefer-const": "error", "block-scoped-var": "error", "prefer-template": "warn", "no-unneeded-ternary": "warn", "no-use-before-define": [ "error", "nofunc" ] } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You will notice that we have some custom rules set up as well. If you have custom rules for your project, this is where you should put them.&lt;/p&gt; &lt;h3&gt;Automating the code linter&lt;/h3&gt; &lt;p&gt;Having a linter in place is great, but it is only effective if you run it. While you can run the &lt;code&gt;eslint&lt;/code&gt; command manually to check your code consistency, remembering to run it that way can become burdensome and error-prone. The best approach is to set up some type of automation.&lt;/p&gt; &lt;p&gt;The first step is to create an npm script like &lt;code&gt;pretest&lt;/code&gt; that will make sure linting happens before your tests are run. That script might look something like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt; "scripts": { "pretest": "eslint --ignore-path .gitignore ." }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Notice that we are telling ESLint to ignore paths that are contained in our &lt;code&gt;.gitignore&lt;/code&gt; file, so make sure the &lt;code&gt;node_modules&lt;/code&gt; folder and other derived files are included in that ignore file. Using an npm script like this easily integrates into most continuous integration (CI) platforms.&lt;/p&gt; &lt;p&gt;Another alternative is to configure hooks so that the linter runs before the code is committed. Libraries like &lt;a href="https://www.npmjs.com/package/husky"&gt;Husky&lt;/a&gt; can help with this workflow. Just be sure that these precommit checks don't take too long, or your developers might complain.&lt;/p&gt; &lt;h3&gt;Conclusion&lt;/h3&gt; &lt;p&gt;It is critical to make sure you enforce consistent code standards across all your projects so that your team can collaborate efficiently. The best way to keep up with that task is to use a linter and automate it as part of your workflow. We recommend ESLint, but you are free to choose whatever tool you want—as long as you have something.&lt;/p&gt; &lt;p&gt;While you wait for the next installment in this series on the Node.js reference architecture, visit the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;GitHub project&lt;/a&gt; to explore sections that might be covered in future articles. If you want to learn more about what Red Hat is up to on the Node.js front, check out &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;our Node.js landing page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/blog/2021/05/07/introduction-to-the-node-js-reference-architecture-part-2-code-consistency" title="Introduction to the Node.js reference architecture, Part 3: Code consistency"&gt;Introduction to the Node.js reference architecture, Part 3: Code consistency&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/CgZ9PJuRfJc" height="1" width="1" alt=""/&gt;</summary><dc:creator>Lucas Holmquist</dc:creator><dc:date>2021-05-17T03:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/05/07/introduction-to-the-node-js-reference-architecture-part-2-code-consistency</feedburner:origLink></entry><entry><title type="html">DevConf.US 2021 - Containers, OpenShift, architecture blueprints, and diagram tooling</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/KdVlenKGAPI/devconf-us-containers-openshift-architecture-blueprints-diagram-tooling.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/85iCFvAjG18/devconf-us-containers-openshift-architecture-blueprints-diagram-tooling.html</id><updated>2021-05-13T05:00:00Z</updated><content type="html">DevConf.US 2021 has kicked off their call for papers this last month and of course it will be a virtual event (hopefully for this one last time) hosted on September 2-3. It's the 4th annual, free, Red Hat sponsored technology conference for community project and professional contributors to Free and Open Source technologies coming to a web browser near you! There is no admission or ticket charge for DevConf.US events. However, you are required to complete a free registration. Talks, presentations and workshops will all be in English. I've put together the following collection of talks as my submissions and happy to preview them here with you. Installing OpenShift as Dev Locally in Minutes! Are you an AppDev that's truly tired of messing with Kubernetes and all those YAML files? Do you have a project in a Git repository and just want to use a container platform for deploying, testing, and running your applications while in the development phase locally on your own machine? Have we got the session for you! Join us for this power session showing you live how easy it is to install the OpenShift Container Platform (latest 4.x version) on your local developer machine in just minutes. Attendees will experience the installation and quick tour of OpenShift Container Platform from the AppDev point of view. Furthermore, they can take it all home as it's hosted in a project online with links and tips for projects that help them to learn how to leverage existing images and products from the provided image registry or Operator Hub catalogs. It's so easy, next year the attendees can return to this conference and share their latest AppDev projects running on their local machines too!  (25 min) Designing your best architecture diagrams  Diagraming is one of the most important communication tools for sharing your project and architectural ideas to your colleagues and teams. In this workshop attendees are walking step-by-step through using an open source tool we host online for designing architecture diagrams like an expert. Attendees work through the following: * open and explore the tooling in your favourite web browser * explore the provided asset libraries for drag-and-drop designing * learn about the three types of diagrams that make up a good design * create your first simple logical diagram * create your first simple schematic diagram * create a detailed diagram * how to export diagrams and elements from a diagram * design tips and tricks Each of the individual labs in this workshop are stand alone, allowing the attendee to focus on anything of interest without having to work through the previous labs. If you're looking to become more proficient in sharing your ideas, architectures, and projects visually to wider audiences you can't underestimate the value of a good diagram. Join us to learn the tips and tricks that make a good diagram such a good communication vehicle and how our tooling eases your design tasks. (workshop)  The workshop is fully implemented and tested with designers and architects around the world and fine tuned to the beginning designer. Workshop is for your previewing pleasure. Real life Retail Architectures in Action We've all had the retail shopping experience, either online or in a physical shop, but have you ever wanted to take a serious look at how they deliver that experience in a cost effective manner at scale? This session takes attendees on a tour of three architecture blueprints covering three of the most interesting solutions retail organisations have to implement successfully to survive. Not only are these architecture solutions interesting, but they are based on successful real life implementations featuring open source technologies and power a lot of your world wide shopping experiences. The following three use cases will be discussed and detailed in architectural diagrams showcasing how open technologies are integrated to solve them: * Supply chain integration * Real-time stock control * Retail data framework The attendee shall depart this session with a working knowledge of how to map general open source technologies to their solutions with examples all based on real life use cases. Material covered is available freely online and attendees can use these solutions as starting points for aligning to their own solution spaces as they can be pre-loaded into our architecture diagram tooling for modification.  Furthermore, content is available online (for example: https://dzone.com/articles/supply-chain-integration-an-architectural-introduc) for each of these use cases providing attendees with reference material post conference. (25 min) I'll keep you posted if these are accepted and share the planning as we move towards this event in the September.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/KdVlenKGAPI" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/85iCFvAjG18/devconf-us-containers-openshift-architecture-blueprints-diagram-tooling.html</feedburner:origLink></entry><entry><title>Why should developers care about GitOps?</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/w9rOGed6im4/why-should-developers-care-about-gitops" /><author><name>Don Schenck</name></author><id>a085c48a-5aaa-4c47-8c83-816334502b91</id><updated>2021-05-13T03:00:40Z</updated><published>2021-05-13T03:00:40Z</published><summary type="html">&lt;p&gt;As a software developer, your primary role is to deliver bits: pieces of executable ones and zeros that work as designed and expected. How do those bits make it into a &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; or virtual machine (VM)? Who cares? &lt;/p&gt; &lt;p&gt;You. You care. I know you do, because developers are wired like that. At some point, obviously, you've observed your code working properly (because all your code is bug-free, right?), so you're very much interested in seeing that it runs the exact same way in testing, staging, and production. &lt;/p&gt; &lt;p&gt;Put it this way: If the code runs on your machine and &lt;em&gt;doesn't&lt;/em&gt; run the same way in testing, your first question is, "What's different?" &lt;/p&gt; &lt;p&gt;GitOps can answer this question. More accurately and importantly, GitOps can help assure you that nothing is different. &lt;/p&gt; &lt;p&gt;Now that &lt;a href="https://www.openshift.com/blog/openshift-pipelines-and-openshift-gitops-are-now-generally-available"&gt;Red Hat OpenShift Pipelines and Red Hat OpenShift GitOps are generally available&lt;/a&gt;, this is a good time to dig into GitOps from the &lt;a href="https://developers.redhat.com/products/openshift/getting-started"&gt;Red Hat OpenShift&lt;/a&gt; developer's point of view. &lt;/p&gt; &lt;h2&gt;What is GitOps?&lt;/h2&gt; &lt;p&gt;Store everything in Git. &lt;/p&gt; &lt;p&gt;Okay, that's a terrible oversimplification, but it is the essence of GitOps: You store your code and infrastructure and build configuration information in your Git repository ("repo"). Tools such as OpenShift Pipelines, ArgoCD, and Kustomize work with and within this concept to make things happen and pull it all together. &lt;/p&gt; &lt;p&gt;Suffice it to say, if you adopt GitOps and all that's associated with this idea and technology, everything becomes code. &lt;/p&gt; &lt;p&gt;Now, as a developer, think about that. Code. This is your wheelhouse; this is where you shine. Imagine being able to control every aspect of development, from testing to writing code to building the solution to deploying it. Suddenly, as a developer, you are in control from beginning to end. &lt;/p&gt; &lt;p&gt;That is not to say operations isn't involved. On the contrary: This means you work with operations to build a complete solution. Developers and operations working together. Developers. Operations. Dev. Ops. &lt;a href="https://developers.redhat.com/topics/devops"&gt;DevOps&lt;/a&gt;! &lt;/p&gt; &lt;h2&gt;Where to start?&lt;/h2&gt; &lt;ul&gt;&lt;li&gt;Try &lt;a href="https://learn.openshift.com/gitops"&gt;these Katacoda scenarios&lt;/a&gt; to learn more. (They run in the browser, so no installations necessary.)&lt;/li&gt; &lt;li&gt;Download our free DevOps book, &lt;a href="https://developers.redhat.com/topics/devops"&gt;&lt;em&gt;DevOps with OpenShift&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Check out &lt;a href="https://developers.redhat.com/blog/tag/gitops"&gt;more GitOps articles at Red Hat Developer&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Software development and operations are merging, and the cloud-native &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD space&lt;/a&gt; is evolving quickly. Now is an excellent time to up your game and automate all the things.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/blog/2021/05/13/why-should-developers-care-about-gitops" title="Why should developers care about GitOps?"&gt;Why should developers care about GitOps?&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/w9rOGed6im4" height="1" width="1" alt=""/&gt;</summary><dc:creator>Don Schenck</dc:creator><dc:date>2021-05-13T03:00:40Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/05/13/why-should-developers-care-about-gitops</feedburner:origLink></entry><entry><title>Receive Side Scaling (RSS) with eBPF and CPUMAP</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/iiAFyLapKg8/receive-side-scaling-rss-with-ebpf-and-cpumap" /><author><name>Lorenzo Bianconi</name></author><id>28bfa072-38c0-456e-996a-180ca32f9d38</id><updated>2021-05-13T03:00:19Z</updated><published>2021-05-13T03:00:19Z</published><summary type="html">&lt;p&gt;High-speed network packet processing presents a challenging performance problem on servers. Modern network interface cards (NICs) can process packets at a much higher rate than the host can keep up with on a single CPU. So, to scale the processing on the host, the &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; kernel sends packets to multiple CPUs using a hardware feature named &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/performance_tuning_guide/network-rss#:~:text=Receive%2DSide%20Scaling%20(RSS)%2C%20also%20known%20as%20multi,be%20processed%20by%20multiple%20CPUs.&amp;text=It%20also%20shows%20how%20many,which%20CPU%20serviced%20the%20interrupt."&gt;Receive Side Scaling&lt;/a&gt; (RSS). RSS relies on a flow hash to spread incoming traffic across the RX IRQ lines, which will be handled by different CPUs. Unfortunately, there can be a number of situations where the NIC hardware RSS features fail; for instance, if the received traffic is not supported by the NIC RSS engine. When RSS is not supported by the NIC, the card delivers all packets to the same RX IRQ line and thus the same CPU. &lt;/p&gt; &lt;p&gt;Previously, if hardware features did not match the deployment use case, there was no good way to fix it. But &lt;a href="https://www.iovisor.org/technology/xdp"&gt;eXpress Data Path&lt;/a&gt; (XDP) offers a high-performance, programmable hook that makes routing to multiple CPUs possible, so the Linux kernel is no longer limited by the hardware. This article shows how to handle this situation in software, with a strong focus on how to solve the issue using XDP and a &lt;a href="https://xdp-project.net/areas/cpumap.html"&gt;CPUMAP&lt;/a&gt; redirect. &lt;/p&gt; &lt;h2&gt;Faster software receive steering with XDP&lt;/h2&gt; &lt;p&gt;The Linux kernel already has some software implementations of RSS called Receive Packet Steering (RPS) and Receive Flow Steering (RFS), but unfortunately, they do not perform well enough to be a replacement for hardware RSS. A faster and more scalable software solution uses XDP to redirect raw frames into a CPUMAP. &lt;/p&gt; &lt;p&gt;XDP is a kernel layer invoked before the normal network stack. This means XDP runs before allocation of the &lt;a href="http://vger.kernel.org/~davem/skb.html"&gt;socket buffer&lt;/a&gt; (SKB), the kernel object that keeps track of the network packet. XDP generally avoids any per-packet memory allocations. &lt;/p&gt; &lt;h2&gt;What is XDP?&lt;/h2&gt; &lt;p&gt;XDP runs an eBPF-program at the earliest possible point in the driver receive path, when DMA rx-ring is synced for the CPU. This eBPF program parses the received frames and returns an action or &lt;em&gt;verdict&lt;/em&gt;, acted on by the networking stack. Possible verdicts are: &lt;/p&gt; &lt;ul&gt;&lt;li&gt;XDP_DROP: Drop the frame, which at the driver level means recycle without alloc.&lt;/li&gt; &lt;li&gt;XDP_PASS: Let the frame pass to normal network stack handling.&lt;/li&gt; &lt;li&gt;XDP_TX: Bounce the frame out of the same interface.&lt;/li&gt; &lt;li&gt;XDP_REDIRECT: The advanced action this article focuses on.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Figure 1 shows the XDP architecture and how XDP interacts with the Linux networking stack. &lt;/p&gt; &lt;figure role="group" data-behavior="caption-replace"&gt;&lt;article&gt;&lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/11/XDP_arch.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/11/XDP_arch.png?itok=ngTpzqy4" width="600" height="326" title="XDP_arch" typeof="Image" /&gt;&lt;/a&gt; &lt;/article&gt;&lt;figcaption class="rhd-media-custom-caption"&gt;Figure 1: XDP in the Linux networking stack.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Redirecting into a CPUMAP&lt;/h2&gt; &lt;p&gt;BPF maps are generic key-value stores and can have different data types. The maps are used both as an interface between a user-space application and an eBPF program running in the Linux kernel, and as a way to pass information to kernel helpers. As of this writing, there are &lt;a href="https://elixir.bootlin.com/linux/v5.10-rc2/source/include/uapi/linux/bpf.h#L130"&gt;28 map types&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;For our use case of software RSS with XDP, the CPUMAP type (&lt;code&gt;BPF_MAP_TYPE_CPUMAP&lt;/code&gt;) is just what we need. The CPUMAP represents the CPUs in the system (zero) indexed as the map-key, and the map-value is the config setting (per CPU map entry). Each CPUMAP entry has a dedicated kernel thread bound to the given CPU to represent the remote CPU execution unit. The following pseudo-code illustrates the allocation of a CPUMAP entry and the related kernel thread: &lt;/p&gt; &lt;pre&gt;static int cpu_map_kthread_run(void *data) { /* do some work */ } int cpu_map_entry_alloc(int cpu, ...) { ... rcpu-&gt;kthread = kthread_create_on_node(cpu_map_kthread_run, ...); kthread_bind(rcpu-&gt;kthread, cpu); wake_up_process(rcpu-&gt;kthread); ... }&lt;/pre&gt;&lt;p&gt; We promised a faster solution with XDP, which is possible only due to the careful design and bulking details happening internally in CPUMAP. These internals are described at the end of the article in the &lt;a href="#details"&gt;Appendix&lt;/a&gt; section. &lt;/p&gt; &lt;h3&gt;Moving raw frames to remote CPUs&lt;/h3&gt; &lt;p&gt;The packet is received on the CPU (the receive CPU) to which the IRQ of the NIC RX queue is steered. This CPU is the one that initially sees the packet, and this is where the XDP program is executed. Because the objective is to scale the CPU usage across multiple CPUs, the eBPF program should use as few cycles as possible on this initial CPU—just enough to determine which remote CPU to send the packet to, and then to use the redirect eBPF helper with a CPUMAP to move the packet to a remote CPU for continued processing. &lt;/p&gt; &lt;p&gt;The remote CPUMAP &lt;code&gt;kthread&lt;/code&gt; receives raw XDP frame (&lt;code&gt;xdp_frame&lt;/code&gt;) objects. Thus, the SKB object is allocated by the remote CPU, and the SKB is passed into the networking stack. The following example illustrates changes to the &lt;code&gt;kthread&lt;/code&gt; pseudo-code to clarify SKB allocation and how SKBs are forwarded to the Linux networking stack: &lt;/p&gt; &lt;pre&gt;static int cpu_map_kthread_run(void *data) { while (!kthread_should_stop()) { ... skb = cpu_map_build_skb(); /* forward to the network stack */ netif_receive_skb_core(skb); ... } }&lt;/pre&gt;&lt;h3&gt;Pitfall: Missing SKB information on the remote CPU&lt;/h3&gt; &lt;p&gt;When an SKB is created based on the &lt;code&gt;xdp_frame&lt;/code&gt; object, certain optional SKB fields are not populated. This is because these fields come from the NIC hardware RX descriptor, and on the remote CPU this RX descriptor is no longer available. The two pieces of hardware "partial-offload" information that commonly go missing are HW RX checksum information (&lt;code&gt;skb-&gt;ip_summed&lt;/code&gt; + &lt;code&gt;skb-&gt;csum&lt;/code&gt;) and the HW RX hash. Less commonly used values that also go missing are the VLAN, RX timestamp, and the mark value. &lt;/p&gt; &lt;p&gt;The missing RX checksum causes a slowdown when transmitting the SKB, because the checksum has to be recalculated. When the network stack needs to access the hash value (see the &lt;code&gt;skb_get_hash()&lt;/code&gt; function) it triggers a software recalculation of the hash. &lt;/p&gt; &lt;h2&gt;New CPUMAP feature: Running XDP on the remote CPU&lt;/h2&gt; &lt;p&gt;Starting from Linux kernel version 5.9 (and soon in &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux 8&lt;/a&gt;) the &lt;a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=9216477449f33cdbc9c9a99d49f500b7fbb81702"&gt;CPUMAP can run a new (second) XDP program on the remote CPU&lt;/a&gt;. This helps scalability because the receive CPU should spend as few cycles as possible per packet. The remote CPU to which the packet is directed can afford to spend more cycles, such as to look deeper into packet headers. The following example shows through pseudo-code what is executed when the eBPF program associated with the CPUMAP entry runs: &lt;/p&gt; &lt;pre&gt;static int cpu_map_bpf_prog_run_xdp(void *data) { ... act = bpf_prog_run_xdp(); switch (act) { case XDP_DROP: ... case XDP_PASS: ... case XDP_TX: ... case XDP_REDIRECT: ... } ... } static int cpu_map_kthread_run(void *data) { while (!kthread_should_stop()) { ... cpu_map_bpf_prog_run_xdp(); ... skb = cpu_map_build_skb(); /* forward to the network stack */ netif_receive_skb_core(skb); ... } } &lt;/pre&gt;&lt;p&gt; This second XDP program, which runs on each remote CPU, is attached by inserting the eBPF program (file descriptor) on a map-entry level. This was achieved by extending the map value, now defined as UAPI via &lt;code&gt;struct bpf_cpumap_val&lt;/code&gt;: &lt;/p&gt; &lt;pre&gt;/* CPUMAP map-value layout * * The struct data-layout of map-value is a configuration interface. * New members can only be added to the end of this structure. */ struct bpf_cpumap_val { __u32 qsize; /* queue size to remote target CPU */ union { int fd; /* prog fd on map write */ __u32 id; /* prog id on map read */ } bpf_prog; }; &lt;/pre&gt;&lt;h2&gt;Practical use case: An issue on low-end hardware&lt;/h2&gt; &lt;p&gt;Some multicore devices on the market do not support RSS. All the interrupts generated by the NICs on such devices are managed by a single CPU (typically CPU0). &lt;/p&gt; &lt;p&gt;However, using XDP and CPUMAPs, it is possible to implement a software approximation of RSS for these devices. By loading an XDP program on the NIC to redirect packets to CPUMAP entries, you can balance the traffic on all available CPUs, executing just a few instructions on the core connected to the NIC IRQ line. The eBPF program running on the CPUMAP entries will implement the logic to redirect the traffic to a remote interface or forward it to the networking stack. Figure 2 shows the system architecture of this solution on the &lt;a href="http://espressobin.net/"&gt;EspressoBin&lt;/a&gt; (mvneta). Most of the code is executed on the CPUMAP entry associated with CPU1. &lt;/p&gt; &lt;figure role="group" data-behavior="caption-replace"&gt;&lt;article&gt;&lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/11/cpumap-test-arch.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/11/cpumap-test-arch.png?itok=n3GqGhx6" width="600" height="245" title="cpumap-test-arch" typeof="Image" /&gt;&lt;/a&gt; &lt;/article&gt;&lt;figcaption class="rhd-media-custom-caption"&gt;Figure 2: How XDP redirects packets off of CPU0 and allows most processing on another CPU.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Future development&lt;/h2&gt; &lt;p&gt;Currently, CPUMAP doesn't call into the generic receive offload (GRO) system, which would boost TCP throughput by creating an SKB that points to several TCP data segments. In order to fill the gap with the SKB scenario, we need to extend CPUMAPs (and XDP in general) with support for jumbo frames, and leverage the GRO code path available in the networking stack. No worries, we are already working on it! &lt;/p&gt; &lt;h2&gt;Acknowledgments&lt;/h2&gt; &lt;p&gt;I would like to thank Jesper Dangaard Brouer and &lt;span class="aCOpRe"&gt;Toke Høiland-Jørgensen&lt;/span&gt; for their detailed contributions and feedback on this article. &lt;/p&gt; &lt;h2&gt;Appendix&lt;/h2&gt; &lt;p&gt;This section explains complexities that will interest some readers but are not necessary to understand the basic concepts discussed in the article. &lt;/p&gt; &lt;h3&gt;Problems with older software receive steering&lt;/h3&gt; &lt;p&gt;The Linux kernel already has a software feature called Receive Packet Steering (RPS) and Receive Flow Steering (RFS), which is logically a software implementation of RSS. This feature is &lt;a href="https://www.kernel.org/doc/html/latest/networking/scaling.html"&gt;hard to configure&lt;/a&gt; and has limited scalability and performance. The performance issue arises because RPS and RFS happen too late in the kernel's receive path, most importantly after the allocation of the SKB. Transferring and queuing these SKB objects to a remote CPU is also a cross-CPU scalability bottleneck that involves interprocessor communication (IPC) calls and moving cache lines between CPUs. &lt;/p&gt; &lt;h3&gt;Pitfall: Incorrect RSS with Q-in-Q VLANs&lt;/h3&gt; &lt;p&gt;When the NIC hardware parser doesn't recognize a protocol, it cannot calculate a proper RX hash and thus cannot do proper RSS across the available RX queues in the hardware (which are bound to IRQ lines). &lt;/p&gt; &lt;p&gt;This particularly applies to new protocols and encapsulations that get developed after the hardware NIC was released. This was very visible when VXLAN was first introduced. To extend some NICs, a firmware upgrade allows them to support new protocols. &lt;/p&gt; &lt;p&gt;Moreover, you would expect NICs to work well with the old, common VLAN (IEEE 802.1Q) protocol standard. They do, except that multiple or stacked VLANs seem to break on many common NICs. The standard for these multiple VLANs is IEEE 802.1ad, informally known as Q-in-Q (incorporated into 802.1Q in 2011). Practical Q-in-Q RSS issues have been seen with the ixgbe and i40e NIC drivers. &lt;/p&gt; &lt;h3&gt;What makes XDP_REDIRECT special?&lt;/h3&gt; &lt;p&gt;The XDP_REDIRECT  verdict is different from the others in that it can queue XDP frame (&lt;code&gt;xdp_frame&lt;/code&gt;) objects into a BPF map. All the other verdicts need to take immediate action, because the &lt;code&gt;xdp_buff&lt;/code&gt; data structure that keeps track of packet data is not allocated anywhere; it is simply a variable in the function call itself. &lt;/p&gt; &lt;p&gt;It is essential, for the sake of performance, to avoid using per-packet allocations to convert the &lt;code&gt;xdp_buff&lt;/code&gt; into an &lt;code&gt;xdp_frame&lt;/code&gt; in order to allow queuing this object executing XDP_REDIRECT. To avoid any memory allocations, the &lt;code&gt;xdp_frame&lt;/code&gt; object is placed in the top headroom of the data packet itself. A CPU prefetch operation runs before the XDP eBPF program, to avoid the overhead of writing into this cache line. &lt;/p&gt; &lt;p&gt;Before returning an XDP_REDIRECT verdict, the XDP eBPF program calls one of the following BPF helpers to describe the redirect destination to which the frame should be sent: &lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;bpf_redirect(ifindex, flags)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;bpf_redirect_map(bpf_map, index_key, flags)&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The first helper simply chooses the Linux network device destination using the &lt;code&gt;ifindex&lt;/code&gt; as a key. The second helper is the big leap that allows users to extend XDP redirect. This helper can redirect into a BPF map at a specific &lt;code&gt;index_key&lt;/code&gt;. This flexibility can be used for CPU steering. &lt;/p&gt; &lt;p&gt;The ability to bulk is important for performance. The map redirect is responsible for creating a bulk effect, because drivers are required to call an &lt;code&gt;xdp_flush&lt;/code&gt; operation when the NAPI poll budget ends. The design allows the individual map-type implementation to control the level of bulking. The next section explains how bulking is used to mitigate the overhead of cross-CPU communication. &lt;/p&gt; &lt;h3&gt;Efficient transfer between CPUs&lt;/h3&gt; &lt;p&gt;The CPUMAP entry represents a multi-producer single-consumer (MPSC) queue (implemented via  &lt;code&gt;ptr_ring&lt;/code&gt; in the kernel). &lt;/p&gt; &lt;p&gt;The &lt;em&gt;single consumer&lt;/em&gt; is the CPUMAP &lt;code&gt;kthread&lt;/code&gt; that can access the &lt;code&gt;ptr_ring&lt;/code&gt; queue without taking any lock. It also tries to bulk dequeue eight &lt;code&gt;xdp_frame&lt;/code&gt; objects, as they represent one cache line. &lt;/p&gt; &lt;p&gt;The &lt;em&gt;multi-producers&lt;/em&gt; can be RX IRQ line CPUs queuing up packets simultaneously for the remote CPU. To avoid queue lock contention for each producer CPU, there is a small eight-object queue to generate bulk enqueueing into the cross-CPU queue. This careful queue usage means that each cache line transfers eight frames across the CPUs.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/blog/2021/05/13/receive-side-scaling-rss-with-ebpf-and-cpumap" title="Receive Side Scaling (RSS) with eBPF and CPUMAP"&gt;Receive Side Scaling (RSS) with eBPF and CPUMAP&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/iiAFyLapKg8" height="1" width="1" alt=""/&gt;</summary><dc:creator>Lorenzo Bianconi</dc:creator><dc:date>2021-05-13T03:00:19Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/05/13/receive-side-scaling-rss-with-ebpf-and-cpumap</feedburner:origLink></entry><entry><title type="html">Simplest Custom Tasks in Kogito</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/inpYyzddsnw/simplest-custom-tasks-in-kogito.html" /><author><name>Kirill Gaevskii</name></author><id>https://blog.kie.org/2021/05/simplest-custom-tasks-in-kogito.html</id><updated>2021-05-12T14:52:50Z</updated><content type="html">As a developer, you can add custom tasks to the designer palette. Customs tasks are process’ activities that allow users to perform custom operations within your process flows. In addition, custom tasks can predefine different visual and execution time properties of the node on the canvas. An example can be input and output parameters, icon, predefined name, documentation, and others. In this example, we will take a look at how to create basic custom tasks from scratch. We will use . GENERATING PROJECT Let’s start with creating a Kogito maven project structure. Copy and paste this command in the console to create a Kogito project: mvn archetype:generate \ -DarchetypeGroupId=org.kie.kogito \ -DarchetypeArtifactId=kogito-quarkus-archetype \ -DgroupId=com.github.hasys -DartifactId=custom-task-sample \ -DarchetypeVersion=1.4.0.Final \ -Dversion=1.0-SNAPSHOT This command will create for us maven project structure with basic Kogito project: . ├── README.md ├── pom.xml └── src ├── main │ ├── java │ │ └── com │ │ └── github │ │ └── hasys │ └── resources │ ├── Traffic Violation.dmn │ ├── application.properties │ └── test-process.bpmn2 └── test ├── java │ └── com │ └── github │ └── hasys │ ├── GreetingsTest.java │ └── TrafficViolationTest.java └── resources └── application.properties We can remove Traffic Violation.dmn and TrafficViolationTest.java as DMN is not part of this example. DEFINING OUR CUSTOM TASK To create a custom service, first, let’s create a Work Item Definition file. It is a plain text file written in MVEL language. The file specifies the task properties and visual effects like icon and default name. You can place a .wid file in the same directory as the BPMN file. The second option is to put a .wid file in a directory with the name global under the root folder. &gt; NOTICE: Keep in mind that only one project root folder can be opened in VS &gt; Code to support global directory. WID files under the global directory will be visible for all BPMN files in the project. Let’s create a WID file customTasks.wid under the global directory. Now our structure looks like this: . ├── README.md ├── global │ └── customTasks.wid ├── pom.xml └── src ├── main │ ├── java │ │ └── com │ │ └── github │ │ └── hasys │ └── resources │ ├── application.properties │ └── test-process.bpmn2 └── test ├── java │ └── com │ └── github │ └── hasys │ └── GreetingsTest.java └── resources └── application.properties We will use all currently supported properties in the WID file. &gt; NOTE: Not all parameters from Business Central are supported in Kogito &gt; anymore. That’s why we recommend removing all unsupported parameters before &gt; migrating to Kogito. Kogito supported parameters are: * name – is a string and represents the name or the ID of the WID. It is an internal name used by the engine to map WID with Work Item Handler Java class * displayName – is a string and represents the human-readable name. This name will be shown to the user in Process Designer on the canvas * icon – is a string and represents the file name. The file can be located in the same directory as the WID file. Also, it is possible to use icon in base64 format * parameters – is an MVEL map and represents the default Data Inputs which will present in Data assignments editor by default * results – is MVEL map and represents default Data Outputs which will present in the Data assignments editor by default * category – is a string and represents a category of the task in the pallet of Process Designer * documentation – is a string and represents the default documentation. This value will present in the Documentation field of the Service task by default [ [ "name" : "CustomTask", "displayName" : "Custom Task", "icon" : "data:image/png;base64,iVBORw0KGgoAAAA...", "category" : "Custom created task", "documentation" : "Basic minimal custom task.", "parameters" : [ "Message" : new StringDataType() ], "results" : [ "Result" : new StringDataType() ] ] ] ADDING SOME LOGIC In Kogito, you can tune your custom task using different mechanisms like . But for our example, we will use the simplest version of the Work Item Handler. This method is backward compatible with Business Central so you can reuse your custom tasks logic. Our Custom Task firstly will print a hello message in the console. Secondly, it will print Data Input parameters we passed from the process to our Work Item Handler. And finally, our custom task will string back from the Handler to the process variable. This result will be printed in the console later in the process using Script Task. First let’s add CustomTaskWorkItemHandler.java under src/main/java/com/github/hasys/customTask/config folder. This is our custom task work item handler and it will contain all our business-specific logic. In the simplest case, we just need to implement executeWorkItem and abortWorkItem methods from KogitoWorkItemHandler. &gt; NOTE: KogitoWorkItemHandler has the same signature as the WorkItemHandler &gt; interface from Business Central. However, have a different name and package so &gt; don’t forget to change it during migration from/to Business Central. Content of our Custom Task handler: package com.github.hasys.customTask.config; import java.util.Map; import java.util.HashMap; import org.kie.kogito.internal.process.runtime.KogitoWorkItem; import org.kie.kogito.internal.process.runtime.KogitoWorkItemHandler; import org.kie.kogito.internal.process.runtime.KogitoWorkItemManager; public class CustomTaskWorkItemHandler implements KogitoWorkItemHandler { @Override public void executeWorkItem(KogitoWorkItem workItem, KogitoWorkItemManager manager) { System.out.println("Hello from the custom work item definition."); System.out.println("Passed parameters:"); // Printing task’s parameters, it will also print // our value we pass to the task from the process for(String parameter : workItem.getParameters().keySet()) { System.out.println(parameter + " = " + workItem.getParameters().get(parameter)); } Map&lt;String, Object&gt; results = new HashMap&lt;String, Object&gt;(); results.put("Result", "Message Returned from Work Item Handler"); // Don’t forget to finish the work item otherwise the process // will be active infinitely and never will pass the flow // to the next node. manager.completeWorkItem(workItem.getStringId(), results); } @Override public void abortWorkItem(KogitoWorkItem workItem, KogitoWorkItemManager manager) { System.err.println("Error happened in the custom work item definition."); } } That is pretty is. The same as it was in the Business Central, but there is one difference between Business Central and Kogito. Before you can start to use it – you must manually register your Work Item Handler in Work Item Handler Configuration. Luckily it can be done very easily, you just need to create a Java class that extends DefaultWorkItemHandlerConfig class. Let’s create CustomWorkItemHandlerConfig Java class under src/main/java/com/github/hasys/customTask/config folder with content: package com.github.hasys.customTask.config; import javax.enterprise.context.ApplicationScoped; import org.kie.kogito.process.impl.DefaultWorkItemHandlerConfig; @ApplicationScoped public class CustomWorkItemHandlerConfig extends DefaultWorkItemHandlerConfig { { register("CustomTask", new CustomTaskWorkItemHandler()); } } EXECUTING OUR TASK The final tree structure of the project will be: . ├── README.md ├── global │ └── customTasks.wid ├── pom.xml └── src ├── main │ ├── java │ │ └── com │ │ └── github │ │ └── hasys │ │ └── customTask │ │ └── config │ │ ├── CustomTaskWorkItemHandler.java │ │ └── CustomWorkItemHandlerConfig.java │ └── resources │ ├── application.properties │ └── test-process.bpmn2 └── test ├── java │ └── com │ └── github │ └── hasys │ └── GreetingsTest.java └── resources └── application.properties Now we are ready to modify the process to use our new Custom Task! Open test-process.bpmn2 and model process as below: To test parameters, we need to create to Process variables: Process Variable contains Tag internal since it will be used internally in the process and there is no need to expose it to the service API. Variable input we will use to get data during REST API call to pass to the Custom task. Link our process variable with Custom Task Data I/O like this: In the last step, we will add a simple script to the script task. This script will check that string value where returned from the Custom task: System.out.println("Result is = " + result); Now we are ready to start and execute our process with our brand new Custom Task! In the console, go to the project root and start the development mode with the following command: mvn clean package quarkus:dev You can use UI to test your REST API if you open the link or you can execute the curl command in the separate console window: curl -X POST "http://localhost:8080/greetings" -H "accept: */*" -H "Content-Type: application/json" -d "{\"input\":\"some value\"}" As you can see, this command has parameter input with some value. This parameter is our Process Variable which will be later passed to our Custom Task! When you trigger our greetings REST API in the console with running the project you will see the following output: Hello from the custom work item definition. Passed parameters: Message = some value TaskName = CustomTask Result is = Message Returned from Work Item Handler As you can see, our Custom task was executed and our input parameters were passed correctly. Also, the result returned to the process and printed by the Script task also without any issues. I hope you enjoyed the reading! And now it will be easy for you to add your custom tasks with custom appearance and behavior to the Kogito projects. Full code of the example you can find. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/inpYyzddsnw" height="1" width="1" alt=""/&gt;</content><dc:creator>Kirill Gaevskii</dc:creator><feedburner:origLink>https://blog.kie.org/2021/05/simplest-custom-tasks-in-kogito.html</feedburner:origLink></entry><entry><title>Using Red Hat's single sign-on technology with external databases, Part 1: Install and configure SSO with MariaDB</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/6SkyIhXi7FI/using-red-hats-single-sign-on-technology-with-external-databases-part-1-install-and-configure-sso-with-mariadb" /><author><name>Issa Gueye</name></author><id>b80e17b7-c748-4b5d-ac18-ab26582725ce</id><updated>2021-05-12T03:00:02Z</updated><published>2021-05-12T03:00:02Z</published><summary type="html">&lt;p&gt;Red Hat's &lt;a href="https://access.redhat.com/announcements/3567891"&gt;single sign-on (SSO) technology&lt;/a&gt;, based on the &lt;a href="https://www.keycloak.org/"&gt;Keycloak&lt;/a&gt; open source project, is Red Hat's solution for securing web applications and RESTful web services. The goal of Red Hat's single sign-on technology is to make security simple, so that it is easy for application developers to secure the apps and services they have deployed in their organization. &lt;/p&gt; &lt;p&gt;Out of the box, single sign-on uses its own Java-based embedded relational database, called H2, to store persistent data. However, this H2 database is not viable in high-concurrency situations and should not be used in a cluster. So, it is highly recommended to replace this H2 database with a more production-ready external database. &lt;/p&gt; &lt;p&gt;This article shows you how to install and configure Red Hat's single sign-on technology to connect to a more mature database. We'll use MariaDB for the example, but the instructions should also work for MySQL. &lt;/p&gt; &lt;p&gt;&lt;b&gt;Note&lt;/b&gt;: This article is the first in a series about using SSO with external databases. My next article will cover deploying an SSO image in &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;. You will learn how to connect the image to an external MariaDB or MySQL server running outside of OpenShift using a custom JDBC driver.&lt;/p&gt; &lt;h2&gt;Setting up and configuring the MariaDB or MySQL database&lt;/h2&gt; &lt;p&gt;To perform the steps in this section, you must first log in as &lt;code&gt;root&lt;/code&gt; on a &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) 7 system. &lt;/p&gt; &lt;h3&gt;Step 1: Install and configure a MariaDB or MySQL database server&lt;/h3&gt; &lt;p&gt;This step can be done through the following commands: &lt;/p&gt; &lt;pre&gt;[root@testvm ~]# yum install mariadb-server mariadb mysql-connector-java [root@testvm ~]# systemctl enable mariadb [root@testvm ~]# systemctl start mariadb [root@testvm ~]# /usr/bin/mysql_secure_installation ... Set root password? [Y/n] Y New password: redhat Re-enter new password: redhat ... Remove anonymous users? [Y/n] Y ... Disallow root login remotely? [Y/n] Y ... Remove test database and access to it? [Y/n] Y ... Reload privilege tables now? [Y/n] Y ... All done! If you've completed all of the above steps, your MariaDB installation should now be secure. Thanks for using MariaDB! [root@testvm ~]# [root@testvm ~]# mysql -h localhost -uroot -predhat ... MariaDB [(none)]&gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | +--------------------+ 3 rows in set (0.01 sec) MariaDB [(none)]&gt; MariaDB [(none)]&gt; quit Bye [root@testvm ~]#&lt;/pre&gt;&lt;h3&gt;Step 2: Create a new user and a database for SSO&lt;/h3&gt; &lt;p&gt;Enter the following commands to create the new user: &lt;/p&gt; &lt;pre&gt;[root@testvm ~]# cat mariadb_rhsso74_db_setup.sql CREATE USER 'rhsso74'@'%' IDENTIFIED BY 'redhat'; DROP DATABASE IF EXISTS `rhsso74`; CREATE DATABASE IF NOT EXISTS `rhsso74`; GRANT ALL PRIVILEGES ON rhsso74.* TO 'rhsso74'@'%' identified by 'redhat'; [root@testvm ~]# [root@testvm ~]# mysql -h localhost -uroot -predhat &lt; mariadb_rhsso74_db_setup.sql [root@testvm ~]# [root@testvm ~]# mysql -h localhost -uroot -predhat MariaDB [(none)]&gt; show databases; +--------------------+ | Database | +--------------------+ | &lt;i&gt;information_schema&lt;/i&gt; | | mysql | | performance_schema | | rhsso74 | +--------------------+ 4 rows in set (0.00 sec) MariaDB [rhsso74]&gt; quit Bye [root@testvm ~]# [root@testvm ~]# mysql -h localhost -urhsso74 -predhat MariaDB [(none)]&gt; show databases; +--------------------+ | Database | +--------------------+ | &lt;i&gt;information_schema&lt;/i&gt; | | rhsso74 | +--------------------+ 2 rows in set (0.00 sec) MariaDB [(none)]&gt; use `rhsso74`; Database changed MariaDB [rhsso74]&gt; show tables; Empty set (0.00 sec) MariaDB [rhsso74]&gt; quit Bye [root@testvm ~]#&lt;/pre&gt;&lt;h2&gt;Installing and configuring SSO&lt;/h2&gt; &lt;p&gt;To perform the steps in this section, log in as &lt;code&gt;ssoadmin&lt;/code&gt; on a RHEL 7 system. &lt;/p&gt; &lt;h3&gt;Step 1: Download and install Red Hat's single sign-on 7.4 software&lt;/h3&gt; &lt;p&gt;At the time of writing this article, &lt;a href="https://access.redhat.com/jbossnetwork/restricted/listSoftware.html?downloadType=distributions&amp;product=core.service.rhsso"&gt;SSO 7.4 is the latest available release&lt;/a&gt;. Download and install SSO under a directory path of your choice, for instance: &lt;/p&gt; &lt;pre&gt;/home/ssoadmin/RH-SSO_SETUP/RH-SSO_7.x/7.4.x/LATEST/ &lt;/pre&gt;&lt;p&gt; After creating the directory, run the following commands to install the software: &lt;/p&gt; &lt;pre&gt;[ssoadmin@testvm LATEST]$ id -a uid=1000(ssoadmin) gid=1000(ssoadmin) groups=1000(ssoadmin),10(wheel) context=system_u:system_r:unconfined_service_t:s0 [ssoadmin@testvm LATEST]$ [ssoadmin@testvm LATEST]$ pwd /home/ssoadmin/RH-SSO_SETUP/RH-SSO_7.x/7.4.x/LATEST/ [ssoadmin@testvm LATEST]$ [ssoadmin@testvm LATEST]$ unzip rh-sso-7.4.0.zip Archive: rh-sso-7.4.0.zip creating: rh-sso-7.4/ creating: rh-sso-7.4/modules/ ... ... inflating: rh-sso-7.4/docs/licenses-rh-sso/org.keycloak,keycloak-server-spi-private,9.0.3.redhat-00002,Apache Software License 2.0.txt [ssoadmin@testvm LATEST]$ [ssoadmin@testvm LATEST]$ cd rh-sso-7.4/ [ssoadmin@testvm rh-sso-7.4]$ [ssoadmin@testvm rh-sso-7.4]$ cat version.txt Red Hat Single Sign-On - Version 7.4.0.GA [ssoadmin@testvm rh-sso-7.4]$&lt;/pre&gt;&lt;h3&gt;Step 2: Add a new admin user and start the SSO server instance&lt;/h3&gt; &lt;p&gt;This step can be done through the following commands: &lt;/p&gt; &lt;pre&gt;[ssoadmin@testvm rh-sso-7.4]$ cd bin [ssoadmin@testvm bin]$ ./add-user.sh --user admin -p redhat Updated user 'admin' to file '/home/ssoadmin/RH-SSO_SETUP/RH-SSO_7.x/7.4.x/LATEST/rh-sso-7.4/standalone/configuration/mgmt-users.properties' Updated user 'admin' to file '/home/ssoadmin/RH-SSO_SETUP/RH-SSO_7.x/7.4.x/LATEST/rh-sso-7.4/domain/configuration/mgmt-users.properties' [ssoadmin@testvm bin]$ [ssoadmin@testvm bin]$ ./standalone.sh&lt;/pre&gt;&lt;h3&gt;Step 3: Change the SSO binding addresses to listen to all network interfaces&lt;/h3&gt; &lt;p&gt;By default, Red Hat's single sign-on technology binds via &lt;a href="https://developers.redhat.com/products/eap/overview"&gt;Red Hat JBoss Enterprise Application Platform&lt;/a&gt; (JBoss EAP) to the localhost loopback address 127.0.0.1. However, that’s not a very useful default setting if you want the authentication server available on your network. So, you need to set up your network interfaces to bind to something other than localhost. You could also do this when starting the server instance by setting the &lt;code&gt;-b&lt;/code&gt; option to the IP bind address of your choice. But it is better to set it in the configuration just once, where it applies to all the network interfaces on the host. &lt;/p&gt; &lt;p&gt;Open a new terminal tab and run the following JBoss command-line interface (CLI) commands: &lt;/p&gt; &lt;pre&gt;[ssoadmin@testvm bin]$ ./jboss-cli.sh --connect [standalone@localhost:9990 /] /interface=public:write-attribute(name=inet-address,value=0.0.0.0) [standalone@localhost:9990 /] /interface=management:write-attribute(name=inet-address,value=0.0.0.0) [standalone@localhost:9990 /] reload [standalone@localhost:9990 /] exit&lt;/pre&gt;&lt;h3&gt;Step 4: Add a new admin user for SSO&lt;/h3&gt; &lt;p&gt;This user account is specific to the server runtime for SSO, and will allow you to log into the &lt;code&gt;master&lt;/code&gt; realm’s administration console to perform administrative tasks in SSO. These tasks include creating realms, creating users, registering clients for applications to be secured by single sign-on, and so on. &lt;/p&gt; &lt;p&gt;When adding the user account, the command parameters differ a little, depending on whether you are using standalone operation mode or domain operation mode. The following works for standalone mode: &lt;/p&gt; &lt;pre&gt;[ssoadmin@testvm bin]$ ./add-user-keycloak.sh -r master -u admin -p redhat Added 'admin' to '/home/ssoadmin/RH-SSO_SETUP/RH-SSO_7.x/7.4.x/LATEST/rh-sso-7.4/standalone/configuration/keycloak-add-user.json', restart server to load user [ssoadmin@testvm bin]$&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If your server is running and accessible from localhost, you can also create this admin user by going to &lt;code&gt;http://localhost:8080/auth&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;For domain mode, you have to point the script to one of your server hosts using the &lt;code&gt;-sc&lt;/code&gt; option. For example: &lt;/p&gt; &lt;pre&gt;[ssoadmin@testvm bin]$ ./add-user-keycloak.sh --sc domain/servers/server-one/configuration -r master -u &lt;em&gt;username&lt;/em&gt; -p &lt;em&gt;password&lt;/em&gt;&lt;/pre&gt;&lt;h3&gt;Step 5: Restart the SSO server instance to load the new admin user&lt;/h3&gt; &lt;p&gt;Restart the SSO server instance: &lt;/p&gt; &lt;pre&gt;[ssoadmin@testvm bin]$ ./standalone.sh ... 22:20:04,239 INFO [org.jboss.as] (MSC service thread 1-1) WFLYSRV0049: Red Hat Single Sign-On 7.4.0.GA (WildFly Core 10.1.2.Final-redhat-00001) starting ... ... 22:20:22,469 INFO [org.keycloak.services] (ServerService Thread Pool -- 68) KC-SERVICES0006: Importing users from '/home/ssoadmin/RH-SSO_SETUP/RH-SSO_7.x/7.4.x/LATEST/rh-sso-7.4/standalone/configuration/keycloak-add-user.json' 22:20:23,322 INFO [org.keycloak.services] (ServerService Thread Pool -- 68) KC-SERVICES0009: Added user 'admin' to realm 'master' ... 22:20:23,948 INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0060: Http management interface listening on &lt;a class="moz-txt-link-freetext" href="http://0.0.0.0:9990/management"&gt;http://0.0.0.0:9990/management&lt;/a&gt; 22:20:23,948 INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0051: Admin console listening on &lt;a class="moz-txt-link-freetext" href="http://0.0.0.0:9990"&gt;http://0.0.0.0:9990&lt;/a&gt; 22:20:23,949 INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0025: Red Hat Single Sign-On 7.4.0.GA (WildFly Core 10.1.2.Final-redhat-00001) started in 19857ms - Started 598 of 893 services (602 services are lazy, passive or on-demand) ...&lt;/pre&gt;&lt;h2&gt;Configuring SSO to use the MariaDB or MySQL database&lt;/h2&gt; &lt;p&gt;These steps, like those in the preceding section, are performed as &lt;code&gt;ssoadmin&lt;/code&gt; on a RHEL 7 system. &lt;/p&gt; &lt;h3&gt;Step 1: Add the MySQL JDBC driver module to SSO&lt;/h3&gt; &lt;p&gt;Enter the following commands for this step: &lt;/p&gt; &lt;pre&gt;[ssoadmin@testvm bin]$ pwd /home/ssoadmin/RH-SSO_SETUP/RH-SSO_7.x/7.4.x/LATEST/rh-sso-7.4/bin [ssoadmin@testvm bin]$ [ssoadmin@testvm bin]$ ./jboss-cli.sh You are disconnected at the moment. Type 'connect' to connect to the server or 'help' for the list of supported commands. [disconnected /] [disconnected /] module add --name=com.mysql --resources=/usr/share/java/mysql-connector-java.jar --dependencies=javax.api,javax.transaction.api [disconnected /] exit [ssoadmin@testvm bin]$ ls -alrt /home/ssoadmin/RH-SSO_SETUP/RH-SSO_7.x/7.4.x/LATEST/rh-sso-7.4/modules/com/mysql/main/ total 868 drwxrwxr-x. 3 ssoadmin ssoadmin 18 Oct 17 19:31 .. -rw-rw-r--. 1 ssoadmin ssoadmin 883899 Oct 17 19:31 mysql-connector-java.jar drwxrwxr-x. 2 ssoadmin ssoadmin 56 Oct 17 19:31 . -rw-rw-r--. 1 ssoadmin ssoadmin 317 Oct 17 19:31 module.xml [ssoadmin@testvm bin]$&lt;/pre&gt;&lt;h3&gt;Step 2: Register the database driver and add the DataSource&lt;/h3&gt; &lt;p&gt;Register the MySQL or MariaDB driver: &lt;/p&gt; &lt;pre&gt;[ssoadmin@testvm bin]$ ./jboss-cli.sh --connect [standalone@localhost:9990 /] [standalone@localhost:9990 /] /subsystem=datasources/jdbc-driver=mysql:add(driver-name=mysql,driver-module-name=com.mysql,driver-xa-datasource-class-name=com.mysql.jdbc.jdbc2.optional.MysqlXADataSource, driver-class-name=com.mysql.jdbc.Driver) {"outcome" =&gt; "success"} [standalone@localhost:9990 /] [standalone@localhost:9990 /] /subsystem=datasources/data-source=KeycloakMariaDBDS:add(jndi-name=java:jboss/datasources/KeycloakMariaDBDS, driver-name=mysql, connection-url=jdbc:mysql://localhost:3306/rhsso74, user-name=rhsso74, password=redhat) {"outcome" =&gt; "success"} [standalone@localhost:9990 /] [standalone@localhost:9990 /] /subsystem=datasources/data-source=KeycloakMariaDBDS:test-connection-in-pool { "outcome" =&gt; "success", "result" =&gt; [true] } [standalone@localhost:9990 /] [standalone@localhost:9990 /] reload [standalone@localhost:9990 /] [standalone@localhost:9990 /] exit [ssoadmin@testvm bin]$&lt;/pre&gt;&lt;h3&gt;Step 3: Update the SSO JPA connection to point to the new DataSource&lt;/h3&gt; &lt;p&gt;You have to stop the server instance for SSO before updating the JPA connection: &lt;/p&gt; &lt;pre&gt;[ssoadmin@testvm bin]$ cp -p /home/ssoadmin/RH-SSO_SETUP/RH-SSO_7.x/7.4.x/LATEST/rh-sso-7.4/standalone/configuration/standalone.xml /home/ssoadmin/RH-SSO_SETUP/RH-SSO_7.x/7.4.x/LATEST/rh-sso-7.4/standalone/configuration/standalone.xml.SAVE_Backup [ssoadmin@testvm bin]$ [ssoadmin@testvm bin]$ vi /home/ssoadmin/RH-SSO_SETUP/RH-SSO_7.x/7.4.x/LATEST/rh-sso-7.4/standalone/configuration/standalone.xml ... &lt;subsystem xmlns="urn:jboss:domain:keycloak-server:1.1"&gt; ... &lt;spi name="connectionsJpa"&gt; &lt;provider name="default" enabled="true"&gt; &lt;properties&gt; &lt;span&gt;&lt;b&gt; &lt;!--&lt;/b&gt;&lt;/span&gt; &lt;property name="dataSource" value="java:jboss/datasources/KeycloakDS"/&gt; &lt;span&gt;&lt;b&gt;--&gt;&lt;/b&gt;&lt;/span&gt; &lt;span&gt; &lt;property name="dataSource" value="java:jboss/datasources/KeycloakMariaDBDS"/&gt;&lt;/span&gt; &lt;property name="initializeEmpty" value="true"/&gt; &lt;property name="migrationStrategy" value="update"/&gt; &lt;property name="migrationExport" value="${jboss.home.dir}/keycloak-database-update.sql"/&gt; &lt;/properties&gt; &lt;/provider&gt; &lt;/spi&gt; ... ... [ssoadmin@testvm bin]$&lt;/pre&gt;&lt;h3&gt;Step 4: Restart SSO&lt;/h3&gt; &lt;p&gt;In this step, you will restart the server instance and check to make sure that its database model is loaded into the database: &lt;/p&gt; &lt;pre&gt;[ssoadmin@testvm bin]$ ./standalone.sh ... 21:14:37,090 INFO [org.jboss.as] (MSC service thread 1-2) WFLYSRV0049: Red Hat Single Sign-On 7.4.0.GA (WildFly Core 10.1.2.Final-redhat-00001) starting ... 21:14:42,034 INFO [org.jboss.as.connector.subsystems.datasources] (MSC service thread 1-6) WFLYJCA0001: Bound data source [java:jboss/datasources/&lt;b&gt;KeycloakMariaDBDS&lt;/b&gt;] ... 21:14:49,568 INFO [org.keycloak.connections.jpa.updater.liquibase.LiquibaseJpaUpdaterProvider] (ServerService Thread Pool -- 67) &lt;span&gt;&lt;b&gt;Initializing database schema. Using changelog META-INF/jpa-changelog-master.xml&lt;/b&gt;&lt;/span&gt; ... 21:15:02,735 INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0025: Red Hat Single Sign-On 7.4.0.GA (WildFly Core 10.1.2.Final-redhat-00001) started in 26903ms - Started 598 of 893 services (602 services are lazy, passive or on-demand)&lt;/pre&gt;&lt;p&gt; Connect to the MariaDB or MySQL database and check the SSO database model: &lt;/p&gt; &lt;pre&gt;[root@testvm ~]# mysql -h localhost -u&lt;span&gt;rhsso74&lt;/span&gt; -p&lt;span&gt;redhat&lt;/span&gt; MariaDB [(none)]&gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | rhsso74 | +--------------------+ 2 rows in set (0.00 sec) MariaDB [(none)]&gt; MariaDB [(none)]&gt; use `rhsso74`; MariaDB [rhsso74]&gt; show tables; +-------------------------------+ | Tables_in_rhsso74 | +-------------------------------+ | ADMIN_EVENT_ENTITY | | ASSOCIATED_POLICY | | AUTHENTICATION_EXECUTION | | AUTHENTICATION_FLOW | | AUTHENTICATOR_CONFIG | | AUTHENTICATOR_CONFIG_ENTRY | | BROKER_LINK | | CLIENT | | CLIENT_ATTRIBUTES | | CLIENT_AUTH_FLOW_BINDINGS | | CLIENT_DEFAULT_ROLES | | CLIENT_INITIAL_ACCESS | | CLIENT_NODE_REGISTRATIONS | | CLIENT_SCOPE | | CLIENT_SCOPE_ATTRIBUTES | | CLIENT_SCOPE_CLIENT | | CLIENT_SCOPE_ROLE_MAPPING | | CLIENT_SESSION | | CLIENT_SESSION_AUTH_STATUS | | CLIENT_SESSION_NOTE | | CLIENT_SESSION_PROT_MAPPER | | CLIENT_SESSION_ROLE | | CLIENT_USER_SESSION_NOTE | | COMPONENT | | COMPONENT_CONFIG | | COMPOSITE_ROLE | | CREDENTIAL | | DATABASECHANGELOG | | DATABASECHANGELOGLOCK | | DEFAULT_CLIENT_SCOPE | | EVENT_ENTITY | | FEDERATED_IDENTITY | | FEDERATED_USER | | FED_USER_ATTRIBUTE | | FED_USER_CONSENT | | FED_USER_CONSENT_CL_SCOPE | | FED_USER_CREDENTIAL | | FED_USER_GROUP_MEMBERSHIP | | FED_USER_REQUIRED_ACTION | | FED_USER_ROLE_MAPPING | | GROUP_ATTRIBUTE | | GROUP_ROLE_MAPPING | | IDENTITY_PROVIDER | | IDENTITY_PROVIDER_CONFIG | | IDENTITY_PROVIDER_MAPPER | | IDP_MAPPER_CONFIG | | KEYCLOAK_GROUP | | KEYCLOAK_ROLE | | MIGRATION_MODEL | | OFFLINE_CLIENT_SESSION | | OFFLINE_USER_SESSION | | POLICY_CONFIG | | PROTOCOL_MAPPER | | PROTOCOL_MAPPER_CONFIG | | REALM | | REALM_ATTRIBUTE | | REALM_DEFAULT_GROUPS | | REALM_DEFAULT_ROLES | | REALM_ENABLED_EVENT_TYPES | | REALM_EVENTS_LISTENERS | | REALM_REQUIRED_CREDENTIAL | | REALM_SMTP_CONFIG | | REALM_SUPPORTED_LOCALES | | REDIRECT_URIS | | REQUIRED_ACTION_CONFIG | | REQUIRED_ACTION_PROVIDER | | RESOURCE_ATTRIBUTE | | RESOURCE_POLICY | | RESOURCE_SCOPE | | RESOURCE_SERVER | | RESOURCE_SERVER_PERM_TICKET | | RESOURCE_SERVER_POLICY | | RESOURCE_SERVER_RESOURCE | | RESOURCE_SERVER_SCOPE | | RESOURCE_URIS | | ROLE_ATTRIBUTE | | SCOPE_MAPPING | | SCOPE_POLICY | | USERNAME_LOGIN_FAILURE | | USER_ATTRIBUTE | | USER_CONSENT | | USER_CONSENT_CLIENT_SCOPE | | USER_ENTITY | | USER_FEDERATION_CONFIG | | USER_FEDERATION_MAPPER | | USER_FEDERATION_MAPPER_CONFIG | | USER_FEDERATION_PROVIDER | | USER_GROUP_MEMBERSHIP | | USER_REQUIRED_ACTION | | USER_ROLE_MAPPING | | USER_SESSION | | USER_SESSION_NOTE | | WEB_ORIGINS | +-------------------------------+ 93 rows in set (0.01 sec) MariaDB [rhsso74]&gt; select ID, NAME, ACCOUNT_THEME, ADMIN_THEME, DEFAULT_LOCALE from REALM; +----------------+----------------+------------------+-------------+----------------+ | ID | NAME | ACCOUNT_THEME | ADMIN_THEME | DEFAULT_LOCALE | +----------------+----------------+------------------+-------------+----------------+ | master | master | keycloak-preview | NULL | NULL | +----------------+----------------+------------------+-------------+----------------+ 1 rows in set (0.00 sec) MariaDB [rhsso74]&gt;&lt;/pre&gt;&lt;h3&gt;Step 5: Recreate the admin user for SSO&lt;/h3&gt; &lt;p&gt;This step is needed because you have switched the database SSO is using to MariaDB or MySQL. The admin user created earlier is in the default H2 database, which is not used anymore: &lt;/p&gt; &lt;pre&gt;[ssoadmin@testvm bin]$ ./add-user-keycloak.sh -r master -u admin -p redhat Added 'admin' to '/home/ssoadmin/RH-SSO_SETUP/RH-SSO_7.x/7.4.x/LATEST/rh-sso-7.4/standalone/configuration/keycloak-add-user.json', restart server to load user [ssoadmin@testvm bin]$ &lt;/pre&gt;&lt;h3&gt;Step 6: Final testing and validation&lt;/h3&gt; &lt;p&gt;Start the SSO server instance: &lt;/p&gt; &lt;pre&gt;[ssoadmin@testvm bin]$ ./standalone.sh ... 22:20:04,239 INFO [org.jboss.as] (MSC service thread 1-1) WFLYSRV0049: Red Hat Single Sign-On 7.4.0.GA (WildFly Core 10.1.2.Final-redhat-00001) starting ... 22:20:22,469 INFO [org.keycloak.services] (ServerService Thread Pool -- 68) KC-SERVICES0006: Importing users from '/home/ssoadmin/RH-SSO_SETUP/RH-SSO_7.x/7.4.x/LATEST/rh-sso-7.4/standalone/configuration/keycloak-add-user.json' 22:20:23,322 INFO [org.keycloak.services] (ServerService Thread Pool -- 68) KC-SERVICES0009: Added user 'admin' to realm 'master' ... 22:20:23,949 INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0025: Red Hat Single Sign-On 7.4.0.GA (WildFly Core 10.1.2.Final-redhat-00001) started in 19857ms - Started 598 of 893 services (602 services are lazy, passive or on-demand)&lt;/pre&gt;&lt;p&gt; Verify that the admin user has been created in the database: &lt;/p&gt; &lt;pre&gt;MariaDB [rhsso74]&gt; select ID, USERNAME, REALM_ID from USER_ENTITY; +--------------------------------------+------------------+----------------+ | ID | USERNAME | REALM_ID | +--------------------------------------+------------------+----------------+ | 075d1cb9-fb4d-4437-80f8-f4397b1b5370 | admin | master | +--------------------------------------+------------------+----------------+ 1 row in set (0.00 sec) MariaDB [rhsso74]&gt;&lt;/pre&gt;&lt;p&gt; Access the admin console in the SSO server instance by opening your browser, loading &lt;code&gt;http://localhost:8080/auth&lt;/code&gt;, and logging in as &lt;code&gt;admin/redhat&lt;/code&gt;. &lt;/p&gt; &lt;h2&gt;Next steps with SSO&lt;/h2&gt; &lt;p&gt;To keep learning about using Red Hat's single sign-on technology, you can play with the &lt;a href="https://github.com/redhat-developer/redhat-sso-quickstarts"&gt;SSO quickstarts&lt;/a&gt; and &lt;a href="https://github.com/keycloak/keycloak-quickstarts"&gt;Keycloak quickstarts&lt;/a&gt; on GitHub. Also, check out the documentation for &lt;a href="https://access.redhat.com/documentation/en/red-hat-single-sign-on/"&gt;Red Hat's single sign-on technology&lt;/a&gt; and the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_single_sign-on/7.4/html/release_notes/index"&gt;SSO 7.4 release&lt;/a&gt;. &lt;/p&gt; &lt;h2&gt;Coming up next ...&lt;/h2&gt; &lt;p&gt;My next article in this series will show you how to deploy the SSO container image in OpenShift. Future articles will cover topics and use cases such as SSO migration and upgrades across different database flavors, and tricks and tips for connecting to an external database using a custom JDBC driver. I'll also share more powerful features of Red Hat's single sign-on technology and other topics related to SSO integration, such as integrating with third-party OpenID Connect and Security Assertion Markup Language (SAML) vendors, clustering while using Nginx as a load balancer or reverse proxy, and using the Proof Key for Code Exchange (PKCE) extension in SSO. Stay tuned!&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/blog/2021/05/12/using-red-hats-single-sign-on-technology-with-external-databases-part-1-install-and-configure-sso-with-mariadb" title="Using Red Hat's single sign-on technology with external databases, Part 1: Install and configure SSO with MariaDB"&gt;Using Red Hat's single sign-on technology with external databases, Part 1: Install and configure SSO with MariaDB&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/6SkyIhXi7FI" height="1" width="1" alt=""/&gt;</summary><dc:creator>Issa Gueye</dc:creator><dc:date>2021-05-12T03:00:02Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/05/12/using-red-hats-single-sign-on-technology-with-external-databases-part-1-install-and-configure-sso-with-mariadb</feedburner:origLink></entry><entry><title type="html">Quarkus 1.13.4.Final released - Maintenance release</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/OkvpEvwdmg0/" /><author><name /></author><id>https://quarkus.io/blog/quarkus-1-13-4-final-released/</id><updated>2021-05-12T00:00:00Z</updated><content type="html">We just released 1.13.4.Final, a new maintenance release for the 1.13 release train. 1.13.4.Final is a safe upgrade for everyone using Quarkus 1.13. If you are not using 1.13 already, please refer to the 1.13 migration guide. What’s new? Full changelog You can get the full changelog of 1.13.4.Final on...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/OkvpEvwdmg0" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://quarkus.io/blog/quarkus-1-13-4-final-released/</feedburner:origLink></entry><entry><title type="html">DashBuilder: Getting Started Guide</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/rY6cfn_YqA8/dashbuilder-getting-started-guide.html" /><author><name>William Siqueira</name></author><id>https://blog.kie.org/2021/05/dashbuilder-getting-started-guide.html</id><updated>2021-05-11T18:59:06Z</updated><content type="html">DashBuilder is a full-featured web application that allows non-technical users and programmers to create business dashboards. Dashboard data can be extracted from heterogeneous sources of information such as Prometheus, JDBC databases, or regular text files. The resulting dashboard can be deployed in a cloud environment using DashBuilder Runtime. In this guide, we will show you how to create your first dashboard using DashBuilder in designer and Java mode. CREATING DASHBOARDS USING DASHBUILDER DESIGNER DashBuilder Designer allows users to visually create dashboards using a web application. In this section, we will show how to get started with this tool. HOW TO START THE DESIGNER? You can run DashBuilder Designer using the following docker command docker run -dp 8080:8080 quay.io/wsiqueir/dashbuilder-webapp:latest Once it started access on the browser and log in as admin/admin DashBuilder Designer contains the tools to create data sets, pages, navigation and at the end, you can export your work to run on a cloud environment. DATA SETS Dashboards need to be fed with data and in DashBuilder, the source of data in your dashboards are data sets. Currently, we support the following data set types: * Prometheus: Generate a data set based on a Prometheus query * Kafka: Generate a data set from Kafka metrics. * Bean: Use to generate a data set from a Java class * CSV: Use to generate a data set from a remote or local CSV file * SQL: Use to generate a data set from an ANSI-SQL compliant database * Elastic Search: Use to generate a data set from Elastic Search nodes * Execution Server: Use to generate a data set using the custom query feature of an Execution Server You can access the Data Sets tool by going to Administration -&gt; Data Sets or click on the Data Sets link on the home page. As an example, you can create a data set using a with game console sales by year and manufacturer. In DashBuilder we create a new data set of type CSV and fill out with the following information: * Name: Any name you want * File URL: (click on the little cloud switch to URL) * Separator Char: , * Quote chart: “ CSV data set creation Once we click on test we have a preview of the data: Previewing data sets See in the left side pane that DashBuilder identifies the column type, but you can force a certain type for a column. In the Preview screen there’s also the Filter tab where it is possible to filter a part of the data set: It is possible to filter a whole data set Clicking on Next will finish the creation wizard and the data set is saved, but it is also possible to tweak cache information in the “Advanced” tab. Once the data set is created you can access it later in the Data Set Explorer tab. Editing an existing data set For more information about CSV data sets check post CONTENT MANAGER Content Manager is the tool that allows you to create pages. To access it go to Administration -&gt; Content Manager or use the Design home menu. Content Manager welcome screen The content manager has two buttons on the top left side which are Pages and Navigation. Let’s talk about pages. PAGES A page is where the actual dashboard lives. A page is composed of rows and columns and finally a component. The component can contain static content, such as HTML or a Logo, or a dynamic component that displays content from a data set. When you click on New Page you are prompted to enter the page name and then you are directed to edit the page. Creating a new page Page Editor The page editor has on the left side components that can be dragged to the page. The components are divided into categories: * Core: Base static components that are not related to dashboards or navigation; * Navigation: Components that allow you to embed other pages inside your page; * Reporting: Dashboard components such as charts and tables; * Heatmaps: Specific components that show a business process heatmap. Page Editor components When you drag a component to the page a dialog with the component properties will appear. This dialog changes according to the component used. Static HTML Component On the right side, you will find the properties of the select component. The properties change according to the select part, which is: page, row, column, or component. The properties are basically CSS properties that allow you to change color, alignment, text, and margin. Component Properties REPORTING COMPONENTS Reporting components pallet Reporting components make use of data sets to display the data in different ways. All these components have a dialog with three tabs: * Type: In this tab, you can dynamically change the component type for the data set you selected * Data: The data that will be used by this component. Once the data set is collected you can configure it according to the displayer you selected * Display: All the chart settings such as size, axis configuration, and more. Taking as an example the “Console Sales” from the Data Sets section, we can create a page called “Console Sales” and on this page, you can drag a “Table” component to see all the data from the data set. Notice also the table has sort and columns selection functionality. Using the Table reporting component Now, see the total(million) and release_year column values, notice that it is formatted as a number. We can change it by editing the component and modifying the column settings. Table configuration The column edition is a powerful feature that allows us to change the column name, evaluate an expression against the column values and apply a formatting pattern for number columns. Columns configuration A group of reporting components is the charts. The most famous type of XY charts are supported by DashBuilder: Bar, Pie, Line, Area, and Bubble: When using a chart we must correctly set up the data set to select which column will be the categories and how to group the other columns to create the Y-axis values. Take for example the data set we created previously and let’s say we want to see the number of consoles sold by each manufacturer. In this case, the manufacturer will be the category and the Y-axis will be the sum of all consoles sales. On a new page called “Consoles by Manufacturer” we can place a bar chart that shows this information: Creating a bar chart You can also extract a single value and show it using the Metric component. In a new page called “Console Sales Summary” we can place two metrics component to show “Consoles Released” and “Total Consoles Sold” metrics: By default the metric will show the value for all manufacturers, in the same ‘Console Sales Summary’, we can add filter components to allow filtering by the manufacturer. The filter components can be found under the Reporting section and it can be displayed as a combo box, or labels for text columns, or a slider for date and number columns. Adding a filter When placing a filter we must make sure that the component to be filtered is listening for filters, this is done in the Filter section of the component settings: Filter configuration Reporting components have also a great property to poll data from a data set. In the Refresh tab, you can set a value in seconds that the component will use to constantly get data from a data set, this is particularly with dynamic data sets such as Prometheus. Auto refresh Finally, if you use components such as Time Series or Heatmap you will notice a different dialog. This happens because these components were built using External Components, which were discussed in  article. Time series component NAVIGATION You can navigate through pages using the navigation tool. It can be accessed also in the Content Manager. It opens in the left side pane: Navigation Editor We can organize our menus using the navigation and it will be displayed when the dashboard is deployed. You can also create groups for pages. Let’s create one for the pages we created in this article: Editing navigation You can embed a single page inside other pages or use Navigation Components to embed a group of pages. Navigation Components Let’s create a new page called “index” to add tab navigation to it so we can navigate to all the pages related to console sales. Adding an index page The page name index is important because later it will be used as the welcome page when we deploy this dashboard. EXPORT AND IMPORT DASHBOARDS DashBuilder supports import/export data in a ZIP format. You can work on a dashboard, export the ZIP and later import it on a clean DashBuilder installation to continue working on it or deploy the dashboard to production using DashBuilder Runtime. The import/export tool can be used using Data Transfer from the home page or using the menu Administration -&gt; Data Transfer. You can select what will be exported using the button “Custom Export” Notice that it is not possible to export pages without its data sets dependencies. Once the dependencies are fixed we can export a ZIP file. So at this point, we are ready to export our work during this article. We should have 4 pages and 1 data set to export. You can also import a ZIP previously exported. To do it use the select a file in the Import section, upload, and then click on Import. Remember to refresh the page after it is done otherwise the changes won’t be visible. CREATING DASHBOARDS USING JAVA If you are a Java developer you must know that DashBuilder also has an API to create dashboards using pure Java. The result is also the ZIP that can either be imported to DashBuilder Designer or be directly deployed in DashBuilder Runtime. The API was covered in the article . Java dashboards creation DASHBOARDS DEPLOYMENT When your work is done and you want to publish it in your company for mass consumption then DashBuilder Designer is not suitable. For this purpose, we have a cloud-native application called DashBuilder Runtime. Last year we talked about it in . To deploy a single dashboard we can use a Docker file that copies a dashboard inside a DashBuilder runtime container. The authentication/authorization can be done with Keycloak or any other SSO solution, but for now, we only add a single admin/admin user: FROM quay.io/wsiqueir/dashbuilder-runtime:latest # adds default admin user RUN /opt/jboss/wildfly/bin/add-user.sh -a -u 'admin' -p 'admin' -g 'admin' ENV LANG en_US.UTF-8 COPY dashboard.zip . CMD ["/opt/jboss/wildfly/bin/standalone.sh", "-b", "0.0.0.0", "-Ddashbuilder.runtime.import=/opt/jboss/dashboard.zip"] hosted with ❤ by The docker file should be placed in the same directory as the ZIP exported from DashBuilder Runtime and the ZIP should be named dashboard.zip. You can build the docker image using the following command with podman or docker: docker build -t console_data_dashboard . Then run the image using the command below and in a few seconds access the dashboard in localhost:8080. Before building the image make sure that the designer’s container is stopped or start DashBuilder runtime in a different port to avoid conflict with port 8080: docker run -dtp 8080:8080 console_data_dashboard Running the dashboard on DashBuilder Runtime CONCLUSION In this guide, we show you how to get started with DashBuilder. If you follow all steps you should be ready to create your own dashboards. Stay tuned to the blog for more news about DashBuilder. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/rY6cfn_YqA8" height="1" width="1" alt=""/&gt;</content><dc:creator>William Siqueira</dc:creator><feedburner:origLink>https://blog.kie.org/2021/05/dashbuilder-getting-started-guide.html</feedburner:origLink></entry><entry><title type="html">Real-time stock control - Common architectural elements</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/2LvnvRVcXpI/real-time-stock-control-common-architectural-elements.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/nGwSUkcpwFc/real-time-stock-control-common-architectural-elements.html</id><updated>2021-05-11T05:00:00Z</updated><content type="html">Part 2 - Common elements In our  from this series we introduced a use case around real-time stock control for retail stores. The process was laid out how we've approached the use case and how portfolio solutions are the base for researching a generic architectural blueprint.  The only thing left to cover was the order in which you'll be led through the blueprint details. This article starts the real journey at the very top, with a generic architecture from which we'll discuss the common architectural elements one by one. This will start our journey into the logical elements that make up the real-time stock control architecture blueprint. BLUEPRINTS REVIEW As mentioned before, the architectural details covered here are base on real solutions using open source technologies. The example scenario presented here is a generic common blueprint that was uncovered researching those solutions. It's our intent to provide a blueprint that provides guidance and not deep technical details. This section covers the visual representations as presented, but it's expected that they'll be evolving based on future research. There are many ways to represent each element in this architectural blueprint, but we've chosen a format that we hope makes it easy to absorb. Feel free to post comments at the bottom of this post, or  with your feedback. FROM SPECIFIC TO GENERIC Before diving in to the common elements, it might be nice to understand that this is not a catch all for every possible supply chain integration solution. It's a collection of identified elements that we've uncovered in multiple customer implementations. These elements presented here are then the generic common architectural elements that we've identified and collected in to the generic architectural blueprint.  It's our intent to provide a blueprint for guidance and not deep technical details. You're smart enough to figure out wiring integration points in your own architectures. You're capable of slotting in the technologies and components you've committed to in the past where applicable.  It's our job here to describe the architectural blueprint generic components and outline a few specific cases with visual diagrams so that you're able to make the right decisions from the start of your own projects. Another challenge has been how to visually represent the architectural blueprint. There are many ways to represent each element, but we've chosen some icons, text and colours that we hope are going to make it all easy to absorb. Now let's take a quick tour of the generic architecture and outline the common elements uncovered in my research. EXTERNAL APPLICATIONS Starting on the left side of the logical diagram you'll find the external applications holding two elements. These are the mobile applications and web applications, used to represent any front end applications used to interact with various aspects of the real-time stock control features.  By including the mobile applications, we're representing the possibility of supporting any type of device from workstations to portable devices that might be used while roaming a retail store location or for example a suppliers remote location. GATEWAYS AND PROXIES As in most organisational architectures, with this blueprint we find a universal need for external system and application access. To ensure that proper authentication and authorisation is applied, most organisations are using API management and some form of reverse proxy setup.  It's not a revelation that these element are in this architecture blueprint, but it would not be a complete story without including them specifically in this architecture elements discussion. CONTAINER PLATFORM The largest collection of elements can be found in the container platform, where processes, services, and rule compliancy tools are provided as independent elements. To support external interaction throughout the real-time stock control platform there are collections of microservices, each focusing on one aspect of the customer interaction.  These are very generic descriptions as each retail organisation can determine what they consider core services.  The first group is referred to as payments microservices that ensure specific focus on consistent integration with the retail organisations preferred payment infrastructure. As payment systems vary widely around the globe and even regionally, it makes sense to ensure a separation of concerns by adding a manageable integration layer in this collection of microservices. Another set of similar services are called the promotions microservices that allow the retail organisation to provide consistent integration with price and supplier fluctuations in stock they are maintaining. Imagine processes that monitor or trigger when stock becomes too great in a certain product line, leveraging these microservices to set promotions to deplete the stockpile more rapidly, all in (near) real-time. Next, there are available to sell microservices providing retail locations and suppliers with consistent integration to stock information, such as depleted stock items, updating stock items, and more. All this can then be accessed by local retail stores for updating their product availability or for picking up newly available products coming into stock. An interesting element is the retail processes where long running processes can be captured, implemented, and leverage tasks for retail associates at the store level. These processes can be tied to stores as they interact with stock control features and also include triggers to actions taken by external vendors, suppliers, and partners. On top of these features, you have reporting and monitoring tooling to help the retail organisation optimise their real-time stock control and processes as data is gathered on what sells, what does not, and how external relationships are performing. A core aspect of this blueprint is the use of event streaming or Event Driven Architecture (EDA). Any time an organisation talks about (near) real-time activities you are heading in the direction of an architecture that must react to events as they happen instead of queuing them for later processing. Using this element the entire architecture blueprint will tie together through the use of events and be driven to react to these events by executing on any of the mentioned microservices or stock control processes. Finally, both integration microservices and integration data microservices are elements that consistently provide access to backend organisational systems, data sources, and other aspects of the retail organisation such as the retail data framework. In other articles, we'll cover that architecture blueprint. You can search for that blueprint on this site for more details. INFRASTRUCTURE SERVICES The next block you see is holding elements known for providing infrastructure services to the real-time stock control systems. These elements in the common architecture were pretty consistent across all of the solutions researched. These tended to be infrastructure elements setup in the retail organisations central location or remote cloud providers with the ability to control communication and overall integration for the complete architecture. The catalog management system element is used to maintain a listing of all the products in stock. The logistics system, supply chain system, and order management system are all used for interaction and management of external vendors, suppliers, and partners.  STORAGE SERVICES The storage services uncovered in this solution space was a fairly diverse, from virtual block storage, container-native storage, and object-based storage used in this solution architecture.   WHAT'S NEXT This was just a short overview of the common generic elements that make up our architecture blueprint for the real-time stock control use case.  An overview of this series on real-time stock control portfolio architecture blueprint: 1. 2. 3. Example stock control architecture Catch up on any past articles you missed by following any published links above. Next in this series, taking a look at an example stock controle architecture for this blueprint. (Article co-authored by , Chief Architect Retail, Red Hat)&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/2LvnvRVcXpI" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/nGwSUkcpwFc/real-time-stock-control-common-architectural-elements.html</feedburner:origLink></entry><entry><title>Build an API using Quarkus from the ground up</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/NeVvAdsCx5g/building-an-api-using-quarkus-from-the-ground-up" /><author><name>Stephen Nimmo</name></author><id>ce9bc46b-a502-44c2-8a94-86315bdab51e</id><updated>2021-05-11T03:00:55Z</updated><published>2021-05-11T03:00:55Z</published><summary type="html">&lt;p&gt;Building a production-ready application has a ton of moving parts. Most of the time, developers create a new project using some sort of tool, like Maven archetypes, and then go from tutorial to tutorial piecing together everything that is needed for their application. This article tries to bring all the parts together and provide a single full reference to the work that needs to be done for a &lt;a href="https://developers.redhat.com/products/quarkus/getting-started"&gt;Quarkus&lt;/a&gt; application. You can find &lt;a href="https://github.com/stephennimmo/quarkus-ground-up/tree/BLOG_POST"&gt;all the examples from this article on GitHub&lt;/a&gt;. &lt;/p&gt; &lt;h2&gt;Tech stack for the Quarkus example&lt;/h2&gt; &lt;p&gt;Here's a quick list of the technologies we will be using: &lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://maven.apache.org/"&gt;Maven&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://quarkus.io/"&gt;Quarkus&lt;/a&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://quarkus.io/guides/rest-json"&gt;RestEasy&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://quarkus.io/guides/hibernate-orm-panache"&gt;Panache&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://quarkus.io/guides/validation"&gt;Bean Validation&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://smallrye.io/"&gt;SmallRye&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://quarkus.io/guides/openapi-swaggerui"&gt;OpenAPI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://mapstruct.org/"&gt;MapStruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://quarkus.io/guides/flyway"&gt;Flyway&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.postgresql.org/"&gt;PostgreSQL&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://junit.org/junit5/docs/current/user-guide/"&gt;JUnit 5&lt;/a&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://assertj.github.io/doc/"&gt;AssertJ&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://testcontainers.org/"&gt;Testcontainers&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Project scope and getting started&lt;/h2&gt; &lt;p&gt;Like any project, we need to start by defining our scope. The goal of this project is to build a customer API. We want to support the basic CRUD (create, read, update, and delete) functionality exposed via a REST API, saving the data to a relational database and publishing data changes to a messaging topic for external asynchronous consumption. The scope for this article is to get an API up and running with functional integration tests. &lt;/p&gt; &lt;p&gt;The following commands create your initial project and start the project in Quarkus &lt;a href="https://quarkus.io/guides/getting-started#development-mode"&gt;dev mode&lt;/a&gt;. This provides a quick validation that the project has been successfully created and is ready to work. &lt;/p&gt; &lt;pre&gt;$ mvn io.quarkus:quarkus-maven-plugin:1.13.3.Final:create \ -DprojectGroupId=dev.rhenergy.customer \ -DprojectArtifactId=customer-api \ -DclassName="dev.rhenergy.customer.CustomerResource" \ -Dpath="/api/customers" $ cd customer-api $ ./mvnw clean quarkus:dev&lt;/pre&gt;&lt;p&gt; Open the project in your IDE of choice and let's get started. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: I have purposely omitted boilerplate code for this article: getters, setters, &lt;code&gt;hashCode&lt;/code&gt;, &lt;code&gt;equals&lt;/code&gt; and &lt;code&gt;toString&lt;/code&gt;, most notably. Additionally, we want to focus a moment on a subtle consideration in our development effort: Keep one eye on your imports. Don't just start importing things. As I add imports, I consciously attempt to limit my exposure to third-party libraries, focusing on staying in the abstraction layers such as the MicroProfile abstractions. Remember that every library you import is now your responsibility to care for and feed.&lt;/p&gt; &lt;h2&gt;Architecture layers: Resource, service, repository&lt;/h2&gt; &lt;p&gt;I like to stick with a traditional Resource Service Repository layering pattern, shown in Figure 1. In this pattern, the &lt;code&gt;Repository&lt;/code&gt; class returns an &lt;code&gt;Entity&lt;/code&gt; object, which is tightly coupled to the underlying database structure. The &lt;code&gt;Service&lt;/code&gt; class accepts and returns &lt;code&gt;Domain&lt;/code&gt; objects and the &lt;code&gt;Resource&lt;/code&gt; layer simply manages the REST concerns, possibly handling additional data transformations from the &lt;code&gt;Domain&lt;/code&gt; object to a specific &lt;code&gt;View&lt;/code&gt; object. &lt;/p&gt; &lt;figure role="group" data-behavior="caption-replace"&gt;&lt;article&gt;&lt;a href="https://developers.redhat.com/sites/default/files/blog/2021/05/img_resource_layers.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2021/05/img_resource_layers.png?itok=mBS-dF6U" width="600" height="210" alt="The architecture for the customer project is broken into the Resource, Service, and Repository layers." title="img_resource_layers" typeof="Image" /&gt;&lt;/a&gt; &lt;figcaption class="rhd-media-caption field__item"&gt; Figure 1: Architecture for the customer project. &lt;/figcaption&gt;&lt;/article&gt;&lt;figcaption class="rhd-media-custom-caption"&gt;Figure 1: Architecture for the customer project.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;I also like to put everything that's related in the same package. In the past, I split out packages into the architectural layers, as follows: &lt;/p&gt; &lt;pre&gt;dev.rhenergy.customer.repository dev.rhenergy.customer.repository.entity dev.rhenergy.customer.resource dev.rhenergy.customer.service&lt;/pre&gt;&lt;p&gt; But as my &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt; got much more focused on a single domain, I now just throw it all in the &lt;code&gt;dev.rhenergy.customer&lt;/code&gt; package, as shown in Figure 2. &lt;/p&gt; &lt;figure role="group" data-behavior="caption-replace"&gt;&lt;article&gt;&lt;a href="https://developers.redhat.com/sites/default/files/blog/2021/05/img_6092f3dcb4600.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2021/05/img_6092f3dcb4600.png?itok=BREnxL7P" width="556" height="280" title="img_6092f3dcb4600" typeof="Image" /&gt;&lt;/a&gt; &lt;/article&gt;&lt;figcaption class="rhd-media-custom-caption"&gt;Figure 2: The full stack of classes all reside in the same domain package.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Quarkus application dependencies&lt;/h2&gt; &lt;p&gt;We'll start our coding with some changes to the &lt;code&gt;pom.xml&lt;/code&gt; file. &lt;/p&gt; &lt;h3&gt;Yaml properties&lt;/h3&gt; &lt;p&gt;The first thing we change is the way properties are managed. I like YAML better than properties files, so I add the following to &lt;code&gt;pom.xml&lt;/code&gt;: &lt;/p&gt; &lt;pre&gt;&lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-config-yaml&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/pre&gt;&lt;p&gt; Then, rename the &lt;code&gt;application.properties&lt;/code&gt;file to &lt;code&gt;application.yaml&lt;/code&gt;. &lt;/p&gt; &lt;h3&gt;Database: Flyway and Panache&lt;/h3&gt; &lt;p&gt;Data interactions are going to be managed by the &lt;a href="https://quarkus.io/guides/hibernate-orm-panache"&gt;Quarkus Panache&lt;/a&gt; extension. We are also going to version our database schema using &lt;a href="https://flywaydb.org/"&gt;Flyway&lt;/a&gt;, which has a &lt;a href="https://quarkus.io/guides/flyway"&gt;Quarkus extension&lt;/a&gt;. To get started, let's add the extensions to the &lt;code&gt;pom.xml&lt;/code&gt; file: &lt;/p&gt; &lt;pre&gt;&lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-hibernate-orm-panache&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-jdbc-postgresql&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-flyway&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/pre&gt;&lt;h3&gt;Flyway&lt;/h3&gt; &lt;p&gt;Using Flyway, we can quickly put together our first table, the &lt;code&gt;customer&lt;/code&gt; table. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: I am taking some liberties with this application. Should the email address be required? Should the phone number be required? Maybe, maybe not.&lt;/p&gt; &lt;p&gt;We will place our first Flyway SQL file in the normal deployment location, &lt;code&gt;src/main/resources/db/migration/V1__customer_table_create.sql&lt;/code&gt;: &lt;/p&gt; &lt;pre&gt;CREATE TABLE customer ( customer_id SERIAL PRIMARY KEY, first_name VARCHAR(100) NOT NULL, middle_name VARCHAR(100), last_name VARCHAR(100) NOT NULL, suffix VARCHAR(100), email VARCHAR(100), phone VARCHAR(100) );&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: There is much discussion about how schema changes should be rolled out in production. For now, we are simply going to let the embedded Flyway library migrate the changes within the application startup. If your application requires more advanced rollouts, such as blue-green or canary, you will need to split the pipelines to have the schema changes roll out independently and be backward compatible.&lt;/p&gt; &lt;h3&gt;JPA with Panache&lt;/h3&gt; &lt;p&gt;Based on the table created with Flyway, we will create our first &lt;code&gt;Entity&lt;/code&gt; object. I use the &lt;a href="https://quarkus.io/guides/hibernate-orm-panache#solution-2-using-the-repository-pattern"&gt;Repository pattern&lt;/a&gt; because I like the extra &lt;code&gt;Repository&lt;/code&gt; interface there. The following goes in a file named &lt;code&gt;src/main/java/dev/rhenergy/customer/CustomerEntity.java&lt;/code&gt;: &lt;/p&gt; &lt;pre&gt;@Entity(name = "Customer") @Table(name = "customer") public class CustomerEntity { @Id @GeneratedValue(strategy = GenerationType.&lt;em&gt;IDENTITY&lt;/em&gt;) @Column(name = "customer_id") private Integer customerId; @Column(name = "first_name") @NotEmpty private String firstName; @Column(name = "middle_name") private String middleName; @Column(name = "last_name") @NotEmpty private String lastName; @Column(name = "suffix") private String suffix; @Column(name = "email") @Email private String email; @Column(name = "phone") private String phone; ... }&lt;/pre&gt;&lt;p&gt; Some notes about the entities: &lt;/p&gt; &lt;ul&gt;&lt;li&gt;I like to name all my Java Persistence API (JPA) entity classes with a suffix &lt;code&gt;Entity&lt;/code&gt;. They serve a purpose: to map back to the database tables. I always provide a layer of indirection between &lt;code&gt;Domain&lt;/code&gt; objects and &lt;code&gt;Entity&lt;/code&gt; objects because when it's missing, I've lost more time than the time I've spent creating and managing the data copying processes.&lt;/li&gt; &lt;li&gt;Because of the way the JPA creates the target object names, you have to explicitly put in the &lt;code&gt;@Entity&lt;/code&gt; annotation so your HQL queries don't have reference &lt;code&gt;CustomerEntity&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;I like to explicitly name both the table and the columns with the &lt;code&gt;@Table&lt;/code&gt; and &lt;code&gt;@Column&lt;/code&gt; annotations. Why? I've lost more time when a code refactor inadvertently breaks the assumed named contracts than the time it costs me to write a few extra annotations.&lt;/li&gt; &lt;li&gt;My database column names are snake_case and the entity's class variables are camelCase.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The repository interface follows: &lt;/p&gt; &lt;pre&gt;@ApplicationScoped public class CustomerRepository implements PanacheRepositoryBase&lt;CustomerEntity, Integer&gt; { }&lt;/pre&gt;&lt;p&gt; It looks simple, but it's got power in all the right places. The file is named &lt;code&gt;CustomerRepository.java&lt;/code&gt;. The &lt;code&gt;PanacheRepositoryBase&lt;/code&gt; interface fills out all the code shown in Figure 3. &lt;/p&gt; &lt;figure role="group" data-behavior="caption-replace"&gt;&lt;article&gt;&lt;a href="https://developers.redhat.com/sites/default/files/blog/2021/05/img_panache_fill.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2021/05/img_panache_fill.png?itok=Hzd4Pgmo" width="600" height="481" alt="When we implement the PanacheRepositoryBase interface, it adds method prototypes to the CustomerEntity class." title="img_panache_fill" typeof="Image" /&gt;&lt;/a&gt; &lt;figcaption class="rhd-media-caption field__item"&gt; Figure 3: Code filled in by the PanacheRepositoryBase interface. &lt;/figcaption&gt;&lt;/article&gt;&lt;figcaption class="rhd-media-custom-caption"&gt;Figure 3: Code filled in by the PanacheRepositoryBase interface.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Domain, then service, then resource&lt;/h2&gt; &lt;p&gt;The customer domain object for this first version of the application is very simple. The following goes in a file named &lt;code&gt;Customer.java&lt;/code&gt;: &lt;/p&gt; &lt;pre&gt;public class Customer { private Integer customerId; @NotEmpty private String firstName; private String middleName; @NotEmpty private String lastName; private String suffix; @Email private String email; private String phone; }&lt;/pre&gt;&lt;h2&gt;Entity-to-domain object mapping&lt;/h2&gt; &lt;p&gt;We need to create mappings between the domain object and the entity object. For these purposes, we will use &lt;a href="https://mapstruct.org/"&gt;MapStruct&lt;/a&gt;. It requires us to add the actual dependency and to enhance the compiler plugin with configuration: &lt;/p&gt; &lt;pre&gt;&lt;mapstruct.version&gt;1.4.2.Final&lt;/mapstruct.version&gt; ... &lt;dependency&gt; &lt;groupId&gt;org.mapstruct&lt;/groupId&gt; &lt;artifactId&gt;mapstruct&lt;/artifactId&gt; &lt;version&gt;${mapstruct.version}&lt;/version&gt; &lt;/dependency&gt; ... &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;${compiler-plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;annotationProcessorPaths&gt; &lt;path&gt; &lt;groupId&gt;org.mapstruct&lt;/groupId&gt; &lt;artifactId&gt;mapstruct-processor&lt;/artifactId&gt; &lt;version&gt;${mapstruct.version}&lt;/version&gt; &lt;/path&gt; &lt;/annotationProcessorPaths&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/pre&gt;&lt;p&gt; The mapper itself is very simple because it's basically a one-to-one mapping between the objects. In the following code, from a file named &lt;code&gt;CustomerMapper.java&lt;/code&gt;, note the additional &lt;code&gt;componentModel = "cdi"&lt;/code&gt; definition. This allows the mappers to get injected: &lt;/p&gt; &lt;pre&gt;@Mapper(componentModel = "cdi") public interface CustomerMapper { CustomerEntity toEntity(Customer domain); Customer toDomain(CustomerEntity entity); }&lt;/pre&gt;&lt;h2&gt;Exception handling&lt;/h2&gt; &lt;p&gt;I usually create a single exception, extending &lt;code&gt;RuntimeException&lt;/code&gt;, and then use that for all my exceptions based on custom logic and for wrapping checked exceptions.  Because only a single exception can be thrown, if I need to customize the response later on in any way, I don't have to write tons of mappers. The following goes in a file named &lt;code&gt;ServiceException.java&lt;/code&gt;: &lt;/p&gt; &lt;pre&gt;public class ServiceException extends RuntimeException public ServiceException(String message) { super(message); } }&lt;/pre&gt;&lt;h2&gt;Build the Service class&lt;/h2&gt; &lt;p&gt;We can now build out the &lt;code&gt;Service&lt;/code&gt; class to handle the CRUD. The following is in a file named &lt;code&gt;CustomerService.java&lt;/code&gt;: &lt;/p&gt; &lt;pre&gt;@ApplicationScoped public class CustomerService { private final CustomerRepository customerRepository; private final CustomerMapper customerMapper; private final Logger logger; public CustomerService(CustomerRepository customerRepository, CustomerMapper customerMapper, Logger logger) { this.customerRepository = customerRepository; this.customerMapper = customerMapper; this.logger = logger; } public List&lt;Customer&gt; findAll(){ return customerRepository.findAll().stream() .map(customerMapper::toDomain) .collect(Collectors.toList()); } public Optional&lt;Customer&gt; findById(Integer customerId) { return customerRepository.findByIdOptional(customerId).map(customerMapper::toDomain); } @Transactional public Customer save(Customer customer) { CustomerEntity entity = customerMapper.toEntity(customer); customerRepository.persist(entity); return customerMapper.toDomain(entity); } @Transactional public Customer update(Customer customer) { if (customer.getCustomerId() == null) { throw new ServiceException("Customer does not have a customerId"); } Optional&lt;CustomerEntity&gt; optional = customerRepository.findByIdOptional(customer.getCustomerId()); if (optional.isEmpty()) { throw new ServiceException(String.format("No Customer found for customerId[%s]", customer.getCustomerId())); } CustomerEntity entity = optional.get(); entity.setFirstName(customer.getFirstName()); entity.setMiddleName(customer.getMiddleName()); entity.setLastName(customer.getLastName()); entity.setSuffix(customer.getSuffix()); entity.setEmail(customer.getEmail()); entity.setPhone(customer.getPhone()); customerRepository.persist(entity); return customerMapper.toDomain(entity); } }&lt;/pre&gt;&lt;p&gt; Notice the &lt;code&gt;@Transactional&lt;/code&gt; annotations for the &lt;code&gt;save&lt;/code&gt; and &lt;code&gt;update&lt;/code&gt; methods. This annotation as it stands is the default behavior, which is to create a new customer record or use an existing one. You will also notice the injected Logger in the constructor as well. The Logger injection is a handy way to include a &lt;a href="http://slf4j.org/manual.html"&gt;Simple Logging Facade for Java&lt;/a&gt; (SLF4J) logger in your classes without have to cut and paste the same code. The &lt;code&gt;LoggerProducer&lt;/code&gt; looks like this: &lt;/p&gt; &lt;pre&gt;@Singleton public class LoggerProducer { @Produces Logger createLogger(InjectionPoint injectionPoint) { return LoggerFactory.getLogger(injectionPoint.getMember().getDeclaringClass().getName()); } }&lt;/pre&gt;&lt;h2&gt;Build the Resource class&lt;/h2&gt; &lt;p&gt;Now let's build out the &lt;code&gt;Resource&lt;/code&gt;. To start us off, we use the &lt;a href="https://www.openapis.org/"&gt;OpenAPI&lt;/a&gt; spec to create a conforming REST API. Let's grab the Quarkus extension for that and put it in our &lt;code&gt;pom.xml&lt;/code&gt; file: &lt;/p&gt; &lt;pre&gt;&lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-smallrye-openapi&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/pre&gt;&lt;p&gt; You can also add extensions via the &lt;a href="https://quarkus.io/guides/maven-tooling"&gt;Quarkus Maven plugin&lt;/a&gt; using the following command. I usually can't remember the names, so I just cut and paste from an example, but it's an option: &lt;/p&gt; &lt;pre&gt;./mvnw quarkus:add-extension -Dextensions="quarkus-smallrye-openapi"&lt;/pre&gt;&lt;p&gt; Because we are going to be serializing objects back and forth using JSON, we also need to add the extension to handle the JSON serialization and deserialization: &lt;/p&gt; &lt;pre&gt;&lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-resteasy-jsonb&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/pre&gt;&lt;h3&gt;Object validation&lt;/h3&gt; &lt;p&gt;We also use the &lt;a href="https://quarkus.io/guides/validation"&gt;Hibernate Bean Validation framework&lt;/a&gt;. This allows you to place &lt;code&gt;@Valid&lt;/code&gt; annotations on the method arguments to trigger the beans'&lt;code&gt; javax.validation.contraints&lt;/code&gt; annotations, such as &lt;code&gt;@NotEmpty&lt;/code&gt; and &lt;code&gt;@Email&lt;/code&gt;: &lt;/p&gt; &lt;pre&gt;&lt;dependency&gt; &lt;groupId&gt;io.quarkus&lt;/groupId&gt; &lt;artifactId&gt;quarkus-hibernate-validator&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/pre&gt;&lt;h3&gt;Resource class&lt;/h3&gt; &lt;p&gt;Here's the &lt;code&gt;Resource&lt;/code&gt; class, with some footnotes. The code goes into the &lt;code&gt;CustomerResource.java&lt;/code&gt; file. &lt;/p&gt; &lt;pre&gt;@Path("/api/customers") @Produces(MediaType.APPLICATION_JSON) @Consumes(MediaType.APPLICATION_JSON) public class CustomerResource { private final CustomerService customerService; private final Logger logger; public CustomerResource(CustomerService customerService, Logger logger) { this.customerService = customerService; this.logger = logger; } @GET @APIResponses( value = { @APIResponse( responseCode = "200", description = "Get All Customers", content = @Content(mediaType = "application/json", schema = @Schema(type = SchemaType.ARRAY, implementation = Customer.class))) } ) public Response get() { return Response.ok(customerService.findAll()).build(); } @GET @Path("/{customerId}") @APIResponses( value = { @APIResponse( responseCode = "200", description = "Get Customer by customerId", content = @Content(mediaType = "application/json", schema = @Schema(type = SchemaType.OBJECT, implementation = Customer.class))), @APIResponse( responseCode = "404", description = "No Customer found for customerId provided", content = @Content(mediaType = "application/json")), } ) public Response getById(@PathParam("customerId") Integer customerId) { Optional&lt;Customer&gt; optional = customerService.findById(customerId); return !optional.isEmpty() ? Response.ok(optional.get()).build() : Response.status(Response.Status.NOT_FOUND).build(); } @POST @APIResponses( value = { @APIResponse( responseCode = "201", description = "Customer Created", content = @Content(mediaType = "application/json", schema = @Schema(type = SchemaType.OBJECT, implementation = Customer.class))), @APIResponse( responseCode = "400", description = "Customer already exists for customerId", content = @Content(mediaType = "application/json")), } ) public Response post(@Valid Customer customer) { final Customer saved = customerService.save(customer); return Response.status(Response.Status.CREATED).entity(saved).build(); } @PUT @APIResponses( value = { @APIResponse( responseCode = "200", description = "Customer updated", content = @Content(mediaType = "application/json", schema = @Schema(type = SchemaType.OBJECT, implementation = Customer.class))), @APIResponse( responseCode = "404", description = "No Customer found for customerId provided", content = @Content(mediaType = "application/json")), } ) public Response put(@Valid Customer customer) { final Customer saved = customerService.update(customer); return Response.ok(saved).build(); } }&lt;/pre&gt;&lt;p&gt; Some notes about the code: &lt;/p&gt; &lt;ul&gt;&lt;li&gt;The &lt;code&gt;@Produces&lt;/code&gt; and &lt;code&gt;@Consumes&lt;/code&gt; annotations can be at the class level, rather than the method level, reducing duplication.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;@APIResponses&lt;/code&gt; definitions are a way to inline the Swagger documentation directly in the code. The annotations generate some noise, but reduce the need to maintain the implementation class separately from the Swagger definition.&lt;/li&gt; &lt;li&gt;Notice that all the methods actually return a &lt;code&gt;Response&lt;/code&gt; object. I find it much easier to manage the &lt;code&gt;Response&lt;/code&gt; data, such as the HTTP status code. If you return an object itself, the framework will automatically wrap it in a 200 response code. This is fine on a &lt;code&gt;GET&lt;/code&gt;, but when I have a &lt;code&gt;POST&lt;/code&gt;, I want to see that pretty 201 response code.&lt;/li&gt; &lt;li&gt;If you use the &lt;code&gt;Response&lt;/code&gt; class, the &lt;code&gt;@ApiResponses&lt;/code&gt; serve as the documentation of the actual payload returned in the body of the response. The OpenAPI extension autogenerates all the Swagger APIs and Swagger UI for you automatically.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Additional OpenAPI documentation&lt;/h2&gt; &lt;p&gt;We can configure the OpenAPI Swagger API and UI in the &lt;code&gt;application.yaml&lt;/code&gt; file as follows: &lt;/p&gt; &lt;pre&gt;mp openapi: extensions: smallrye: info: title: Customer API version: 0.0.1 description: API for retrieving customers contact: email: techsupport@rhenergy.dev name: Customer API Support url: http://rhenergy.github.io/customer-api license: name: Apache 2.0 url: http://www.apache.org/licenses/LICENSE-2.0.html&lt;/pre&gt;&lt;h2&gt;Testing the application&lt;/h2&gt; &lt;p&gt;Now we have the full stack in place. Let's get to the tests. First add the AssertJ library to the &lt;code&gt;pom.xml&lt;/code&gt; file. Fluent assertions allow for more condensed test assertions and reduces the amount of repetitive code: &lt;/p&gt; &lt;pre&gt;&lt;dependency&gt; &lt;groupId&gt;org.assertj&lt;/groupId&gt; &lt;artifactId&gt;assertj-core&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/pre&gt;&lt;p&gt; Now on to the test, in a file named &lt;code&gt;CustomerResourceTest.java&lt;/code&gt;: &lt;/p&gt; &lt;pre&gt;@QuarkusTest public class CustomerResourceTest { @Test public void getAll() { given() .when().get("/api/customers") .then() .statusCode(200); } @Test public void getById() { Customer customer = createCustomer(); Customer saved = given() .contentType(ContentType.JSON) .accept(ContentType.JSON) .body(customer) .post("/api/customers") .then() .statusCode(201) .extract().as(Customer.class); Customer got = given() .when().get("/api/customers/{customerId}", saved.getCustomerId()) .then() .statusCode(200) .extract().as(Customer.class); assertThat(saved).isEqualTo(got); } @Test public void post() { Customer customer = createCustomer(); Customer saved = given() .contentType(ContentType.JSON) .accept(ContentType.JSON) .body(customer) .post("/api/customers") .then() .statusCode(201) .extract().as(Customer.class); assertThat(saved.getCustomerId()).isNotNull(); } @Test public void postFailNoFirstName() { Customer customer = createCustomer(); customer.setFirstName(null); given() .contentType(ContentType.JSON) .accept(ContentType.JSON) .body(customer) .post("/api/customers") .then() .statusCode(400); } @Test public void put() { Customer customer = createCustomer(); Customer saved = given() .contentType(ContentType.JSON) .accept(ContentType.JSON) .body(customer) .post("/api/customers") .then() .statusCode(201) .extract().as(Customer.class); saved.setFirstName("Updated"); Customer updated = given() .contentType(ContentType.JSON) .accept(ContentType.JSON) .body(saved) .put("/api/customers") .then() .statusCode(200) .extract().as(Customer.class); assertThat(updated.getFirstName()).isEqualTo("Updated"); } @Test public void putFailNoLastName() { Customer customer = createCustomer(); Customer saved = given() .contentType(ContentType.JSON) .accept(ContentType.JSON) .body(customer) .post("/api/customers") .then() .statusCode(201) .extract().as(Customer.class); saved.setLastName(null); given() .contentType(ContentType.JSON) .accept(ContentType.JSON) .body(saved) .put("/api/customers") .then() .statusCode(400); } private Customer createCustomer() { Customer customer = new Customer(); customer.setFirstName(RandomStringUtils.randomAlphabetic(10)); customer.setMiddleName(RandomStringUtils.randomAlphabetic(10)); customer.setLastName(RandomStringUtils.randomAlphabetic(10)); customer.setEmail(RandomStringUtils.randomAlphabetic(10) + "@rhenergy.dev"); customer.setPhone(RandomStringUtils.randomNumeric(10)); return customer; } }&lt;/pre&gt;&lt;p&gt; By no means do I consider this set of tests complete, but it does a decent job with the vanilla use cases. When you try to run the test, you'll quickly realize that we haven't wired up the needed database. We'll address that problem next. &lt;/p&gt; &lt;h2&gt;Integration testing with Testcontainers&lt;/h2&gt; &lt;p&gt;The 1.13 release of Quarkus includes a new set of tools known as &lt;a href="https://quarkus.io/guides/datasource#devservices-configuration-free-databases"&gt;devservices&lt;/a&gt;. With devservices, the Quarkus app automatically spins up a new &lt;a href="https://www.testcontainers.org/modules/databases/"&gt;Testcontainer&lt;/a&gt; for the appropriate database based on the inclusion of a specific &lt;code&gt;quarkus-jdbc&lt;/code&gt; extension in the project. This wires up all the infrastructure needed to run a PostgreSQL container in the background when the JUnit tests start. But there's a missing piece: the application configuration. The following is a subset of the full &lt;code&gt;application.yaml&lt;/code&gt; file, showing just the part related to the database: &lt;/p&gt; &lt;pre&gt;quarkus: banner: enabled: false datasource: db-kind: postgresql devservices: image-name: postgres:13 hibernate-orm: database: generation: none "%dev": quarkus: log: level: INFO category: "dev.rhenergy": level: DEBUG hibernate-orm: log: sql: true flyway: migrate-at-start: true locations: db/migration,db/testdata "%test": quarkus: log: level: INFO category: "dev.rhenergy": level: DEBUG hibernate-orm: log: sql: true flyway: migrate-at-start: true locations: db/migration,db/testdata&lt;/pre&gt;&lt;p&gt; The only adjustment made to the devservices is to specify the use of a particular Postgres container. &lt;/p&gt; &lt;p&gt;Now run your tests: &lt;/p&gt; &lt;pre&gt;./mvnw clean test &lt;/pre&gt;&lt;p&gt; The tests compile and start a fully running application. The tests are running against the actual HTTP endpoints. &lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;Hopefully, this article gives you a good idea of the scope of building a new REST API from scratch. Future articles will go through the complete Quarkus development life cycle and show the full deployment to the production environment, with everything that entails. Happy coding.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/blog/2021/05/11/building-an-api-using-quarkus-from-the-ground-up" title="Build an API using Quarkus from the ground up"&gt;Build an API using Quarkus from the ground up&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/NeVvAdsCx5g" height="1" width="1" alt=""/&gt;</summary><dc:creator>Stephen Nimmo</dc:creator><dc:date>2021-05-11T03:00:55Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2021/05/11/building-an-api-using-quarkus-from-the-ground-up</feedburner:origLink></entry></feed>
